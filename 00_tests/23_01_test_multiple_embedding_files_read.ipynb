{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom getstate functions for dwave.cloud.config.models.ClientConfig, dwave.cloud.client.qpu.Client, dwave.cloud.solver.StructuredSolver, dwave.system.samplers.dwave_sampler.DWaveSampler, dwave.system.composites.embedding.FixedEmbeddingComposite have been initialized.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pathlib\n",
    "sys.path.append(str(pathlib.PurePath(pathlib.Path.cwd().parent)))\n",
    "\n",
    "import os\n",
    "import multiprocessing\n",
    "import time\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "import dimod\n",
    "import dwave\n",
    "import dwave.system\n",
    "from dwave.system import DWaveSampler, EmbeddingComposite, FixedEmbeddingComposite\n",
    "import dwave.inspector\n",
    "import dwave_networkx as dnx\n",
    "import minorminer\n",
    "\n",
    "from src.particle_funcs import distance_matrix as distance_matrix\n",
    "from src.particle_funcs import io as particles_io\n",
    "import src.leap_funcs.qubo.q_matrix as q_matrix\n",
    "\n",
    "from src import leap_funcs as leap_funcs\n",
    "from src.leap_funcs import embedding_quality\n",
    "\n",
    "from src import h5py_funcs\n",
    "from src.h5py_funcs import discoveries, init_custom_getstates, io, parameterstudy_using_info_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folder_path_main = 'test_multi_param_emb_thr'\n",
    "#info_file_name = 'info.h5'\n",
    "#folder_embeddings = 'embeddings'\n",
    "#embeddings_meta_file_name = '00_meta'\n",
    "#folder_results = 'results'\n",
    "#\n",
    "#meta_file_name_path = os.path.join(folder_path_main, folder_embeddings, embeddings_meta_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_info_file(file_name_path='', infoset_name: str = ''):\n",
    "    return h5py_funcs.io.read_info_from_hdf5_file(file_name_path=file_name_path, infoset_name=infoset_name)\n",
    "\n",
    "def read_embeddings(reread_info_file = {}, folder_path_main=''):\n",
    "    import ast\n",
    "    embeddings = {}\n",
    "    for key, file_names in reread_info_file['info']['embs']['file_names'].items():\n",
    "        #print(key, file_names)\n",
    "        for ind, file in enumerate(file_names['data']):\n",
    "            file = file.decode('utf-8')\n",
    "            file_name_path_emb = os.path.join(folder_path_main,'embeddings', file)\n",
    "            emb = h5py_funcs.io.read_info_from_hdf5_file(\n",
    "                file_name_path=file_name_path_emb, infoset_name='embedding_mm_01/embedding') \n",
    "            emb = {ast.literal_eval(var): list(qs['data']) for var, qs in emb.items()}\n",
    "            embeddings.update({file: emb})\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_embedding(rocky_analysis_path='', nearest_neighbours=0, folder_name_embs='', file_name_emb=''):\n",
    "\n",
    "    #folder_path = pathlib.Path(r'C:\\zz_tmp_OAH\\perf_sphere\\qdem_sub_1\\01_Particles_0005_CPU_4_GPU_0.rocky.files')\n",
    "    folder_path = pathlib.Path(rocky_analysis_path)\n",
    "    print(folder_path)\n",
    "    dem_data = particles_io.read_dem_data(folder_path=folder_path)\n",
    "\n",
    "    time_values_for_snapschots = (57.5, 60)\n",
    "    filenames_for_snapshots = []\n",
    "    num_particles = None\n",
    "    particles_coords_names = None\n",
    "    for key, value in dem_data.items():\n",
    "        if key not in ('num_particles', 'particle_index'):\n",
    "            pass\n",
    "        else:\n",
    "            continue\n",
    "        #print(value['attrs']['time_value'][0])\n",
    "        if np.isclose(value['attrs']['time_value'][0], time_values_for_snapschots).any():\n",
    "            print(value['attrs']['time_index'], value['attrs']['time_value'])\n",
    "            print(key)\n",
    "            filenames_for_snapshots.append(key)\n",
    "    num_particles = dem_data['num_particles'].astype(int)\n",
    "    particles_coords_names = dem_data[filenames_for_snapshots[0]]['Particles']['particles_position']['data'].dtype.names\n",
    "    print(filenames_for_snapshots, num_particles)\n",
    "    print(particles_coords_names)\n",
    "    print(type(num_particles))\n",
    "\n",
    "    part_coords_n = dem_data[filenames_for_snapshots[1]]['Particles']['particles_position']['data'].view((np.double, len(particles_coords_names)))\n",
    "    part_coords_nm1 = dem_data[filenames_for_snapshots[0]]['Particles']['particles_position']['data'].view((np.double, len(particles_coords_names)))\n",
    "    #part_coords_n\n",
    "\n",
    "    distances = distance_matrix.calc_phi_ij(part_coords_n, part_coords_nm1)\n",
    "    #distances\n",
    "\n",
    "    Q_dist_diag = q_matrix.q_dist_diag(distances)\n",
    "    Q_part = q_matrix.q_part(np.shape(distances)[0])\n",
    "    Q_pos = q_matrix.q_pos(np.shape(distances)[0])\n",
    "    Q_array = Q_dist_diag + Q_part + Q_pos\n",
    "    Q_dict = q_matrix.Q_convert_to_dict(Q_array)\n",
    "    del Q_dict\n",
    "    Q_dict_new = q_matrix.Q_convert_to_dict_new_keys_iter(Q_array, num_particles)\n",
    "    if num_particles != nearest_neighbours:\n",
    "        Q_dict_new = q_matrix.reduce_dict_to_nearest_neighbours(Q_dict_new, distances, nearest_neighbours)\n",
    "\n",
    "    with open('../API_Token_Oliver_Dev.txt') as file:\n",
    "        token = file.readline().rstrip()\n",
    "        architecture = file.readline().rstrip()\n",
    "\n",
    "    sampler = DWaveSampler(token = token, architecture='pegasus', region='eu-central-1')\n",
    "    tmp_not_needed_as_a_variable = sampler.adjacency # required for sampler having all data needed for __getstate__, no idea why this is necessary\n",
    "    sampler_graph = sampler.to_networkx_graph()\n",
    "\n",
    "    sampler_dict = {}\n",
    "    _tmp = sampler.client.config.dict().copy()\n",
    "    _tmp.update({'token': 'removed_for_privacy'})\n",
    "    sampler_dict.update({'client_reduced': _tmp.copy()})\n",
    "    #sampler_dict.update({'solver_reduced': \n",
    "    _tmp = sampler.solver.data.copy()\n",
    "    for key in list(_tmp['properties'].keys()):\n",
    "        if key not in ('num_qubits', 'qubits', 'couplers', 'chip_id', 'tags', 'topology', 'category'):\n",
    "            _tmp['properties'].pop(key)\n",
    "    sampler_dict.update({'solver_reduced': _tmp.copy()})\n",
    "    del _tmp\n",
    "\n",
    "    logic_vars = set(elem[0] for elem in list(Q_dict_new.keys()))\n",
    "    layout_source = {elem:tuple(part_coords_n[elem[0]-1][:2]) for elem in logic_vars}\n",
    "    #print(logic_vars)\n",
    "    #print(layout_source)\n",
    "\n",
    "\n",
    "    kwargs_diffusion_candidates = {}\n",
    "    #kwargs_diffusion_candidates_01 = {\n",
    "    #    'tries':20, \n",
    "    #    'verbose':1,\n",
    "    #    'layout':layout_source,\n",
    "    #    #'vicinity':3,\n",
    "    #    #'viscosity':,\n",
    "    #    #'delta_t':,\n",
    "    #    #'d_lim':,\n",
    "    #    #'downscale':,\n",
    "    #    #'keep_ratio':,\n",
    "    #    #'expected_occupancy':\n",
    "    #    }\n",
    "    #kwargs_diffusion_candidates_02 = kwargs_diffusion_candidates_01.copy()\n",
    "    #kwargs_diffusion_candidates_02.update({'tries':50})\n",
    "    #kwargs_diffusion_candidates = {'mm_01': None, 'mm_02': None, 'em_01':kwargs_diffusion_candidates_01, 'em_02':kwargs_diffusion_candidates_02}\n",
    "    kwargs_diffusion_candidates = {}\n",
    "    kwargs_minorminer = {}\n",
    "    kwargs_minorminer_mm_01 = {\n",
    "    #        'max_no_improvement':250,\n",
    "    #        #random_seed=None,\n",
    "    #        #timeout=1000,\n",
    "    #        #max_beta=None,\n",
    "    #        'tries':250,\n",
    "    #        #inner_rounds=None,\n",
    "    #        'chainlength_patience':250,\n",
    "    #        #max_fill=None,\n",
    "    #        #threads=1,\n",
    "    #        #return_overlap=False,\n",
    "    #        #skip_initialization=False,\n",
    "    #        #verbose=0,\n",
    "    #        #interactive=False,\n",
    "    #        #initial_chains=(),\n",
    "    #        #fixed_chains=(),\n",
    "    #        #restrict_chains=(),\n",
    "    #        #suspend_chains=(),\n",
    "    }\n",
    "    #kwargs_minorminer_mm_02 = kwargs_minorminer_mm_01.copy()\n",
    "    #kwargs_minorminer_mm_02.update({'max_no_improvement':500, 'tries':500, 'chainlength_patience':500})\n",
    "    #kwargs_minorminer = {'mm_01':kwargs_minorminer_mm_01, 'mm_02':kwargs_minorminer_mm_02}\n",
    "    kwargs_minorminer = {'mm_01':kwargs_minorminer_mm_01}\n",
    "\n",
    "    kwargs_embera = {}\n",
    "    #kwargs_embera.update({'em_01': None, 'em_02': None})\n",
    "\n",
    "    timings_candidates = {}\n",
    "    timings_embedding = {}\n",
    "    #kwargs_wo_layout = kwargs_diffusion_candidates_01.copy()\n",
    "    #kwargs_wo_layout.pop('layout')\n",
    "    #timings_candidates.update({'mm_01':None, 'mm_02':None, 'em_01':None, 'em_02':None})\n",
    "\n",
    "\n",
    "\n",
    "    embeddings = {}\n",
    "    for key in kwargs_minorminer.keys():\n",
    "        tic = time.time()\n",
    "        embedding_mm = minorminer.find_embedding(S=Q_dict_new, T=sampler_graph, interactive=True, verbose=1, **kwargs_minorminer[key])\n",
    "        toc = time.time()\n",
    "        timings_embedding[key] = toc-tic\n",
    "        embeddings[key] = {'embedding': embedding_mm, 'sampler': sampler_dict}\n",
    "    #tic = time.time()\n",
    "    #candidates_em_layout = embera.preprocess.diffusion_placer.find_candidates(Q_dict_edgelist, sampler_graph, **kwargs_diffusion_candidates)\n",
    "    #toc = time.time()\n",
    "    #timings_candidates['em_layout'] = toc-tic\n",
    "\n",
    "\n",
    "\n",
    "    composites_fixed = {}\n",
    "    for key, emb_sam in embeddings.items():\n",
    "        sampler = DWaveSampler(token = token, architecture='pegasus', region='eu-central-1')\n",
    "        tmp_not_needed_as_a_variable = sampler.adjacency # required for sampler having all data needed for __getstate__, no idea why this is necessary\n",
    "        composites_fixed[key] = FixedEmbeddingComposite(sampler, emb_sam['embedding'])\n",
    "\n",
    "    params_sampling = {'label' : 'superdupernice label',\n",
    "              'annealing_time': 20, \n",
    "              'num_reads': 1000, \n",
    "              'answer_mode': 'raw', \n",
    "              'programming_thermalization': 1000, \n",
    "              'readout_thermalization': 0\n",
    "              }\n",
    "\n",
    "    for key, value in composites_fixed.items():\n",
    "        emb = value.embedding\n",
    "        #print(emb)\n",
    "        num_qubits = len(set(inner for outer in emb.values() for inner in outer))\n",
    "        estimate_time_ys = value.child.solver.estimate_qpu_access_time(num_qubits=num_qubits, **params_sampling)\n",
    "        print('{:22s}'.format(key), num_qubits, estimate_time_ys*1e-6)\n",
    "\n",
    "\n",
    "\n",
    "    #folder_path = 'test_embeddings'\n",
    "    #file_name = 'embeddings'\n",
    "    file_name_path = os.path.join(folder_name_embs, file_name_emb)\n",
    "\n",
    "\n",
    "    kwargs_file_writing_meta={'file_name_path': file_name_path,\n",
    "                         'overwrite_data_in_file': False,\n",
    "                         'track_order': True}\n",
    "\n",
    "    meta_to_write = {\n",
    "            'kwargs_diffusion_candidates': kwargs_diffusion_candidates, \n",
    "            'kwargs_minorminer': kwargs_minorminer,\n",
    "            'kwargs_embera': kwargs_embera,\n",
    "            'timings_candidates': timings_candidates,\n",
    "            'timings_embedding': timings_embedding\n",
    "            }\n",
    "    h5py_funcs.io.write_to_hdf5_file(dict_data=meta_to_write, data_name='embedding', name_suffix='_00_meta', **kwargs_file_writing_meta)\n",
    "\n",
    "    for key, emb in embeddings.items():\n",
    "        print(emb)\n",
    "        emb_to_write = emb\n",
    "        meta_to_write_emb = {key_meta: value_meta[key] for key_meta, value_meta in meta_to_write.items() if key in value_meta}\n",
    "        kwargs_file_writing_embs = kwargs_file_writing_meta.copy()\n",
    "        #kwargs_file_writing_embs.update({'file_name_path': os.path.join(folder_path, key+'.h5')})\n",
    "        h5py_funcs.io.write_to_hdf5_file(dict_data=emb_to_write, data_name='embedding', name_suffix='_'+key, **kwargs_file_writing_embs)\n",
    "        h5py_funcs.io.write_to_hdf5_file(dict_data=meta_to_write_emb, data_name='embedding', name_suffix='_'+key+'_meta', **kwargs_file_writing_embs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _main_find_embedding(reread_info_file={}, folder_path_main=''):\n",
    "    for key, dem in reread_info_file['info']['DEMs']['rocky_files'].items():\n",
    "        rocky_files_path = dem['data'].decode('utf-8')+r'.files'\n",
    "        print(rocky_files_path)\n",
    "        for ind, nearest_neighbours in enumerate(reread_info_file['info']['nearest_neighbours'][key]['data']):\n",
    "            file_name_emb = reread_info_file['info']['embs']['file_names'][key]['data'][ind].decode('utf-8')\n",
    "            print(file_name_emb)\n",
    "            folder_name_embs = os.path.join(folder_path_main,'embeddings')\n",
    "            _find_embedding(rocky_analysis_path=rocky_files_path, nearest_neighbours=nearest_neighbours, folder_name_embs=folder_name_embs, file_name_emb=file_name_emb)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overloaded_submitter_work(self, problem, verbose=0, print_prefix=''):\n",
    "    print(print_prefix + 'start working on problem {}', problem)\n",
    "    identifier = [problem.keys()][0]\n",
    "    embedding=problem['embedding']\n",
    "    sampler = DWaveSampler(**self.solver)\n",
    "    tmp_not_needed_as_a_variable = sampler.adjacency # required for sampler having all data needed for __getstate__, no idea why this is necessary\n",
    "    composite = FixedEmbeddingComposite(child_sampler=sampler, embedding=embedding)\n",
    "    num_qubits = len(set(inner for outer in embedding.values() for inner in outer))\n",
    "    estimate_time_ys = composite.child.solver.estimate_qpu_access_time(num_qubits=num_qubits, **problem['kwargs_sampling'])\n",
    "    _tmp = np.ceil(estimate_time_ys, 1e6)\n",
    "    num_reads = [problem['kwargs_sampling']['num_reads']]\n",
    "    if _tmp > 1:\n",
    "        print(print_prefix + 'problem {} will take too long ({}) to solve'.format(problem, estimate_time_ys))\n",
    "        problem['num_runs']\n",
    "        n, r = divmod(num_reads, _tmp)\n",
    "        num_reads = [n]*_tmp\n",
    "        num_reads[0] += r\n",
    "    print(print_prefix + 'problem {} will be solved {} times'.format(problem, num_reads))\n",
    "    dict_data = {'set_identifier': identifier, 'composite': composite.__getstate__().copy()}\n",
    "    \n",
    "    self.lock_info_file.acquire()\n",
    "    h5py_funcs.parameterstudy_using_info_file.update_timestamp_in_info_file(file_name_path=os.path.join(self.folder_path_main, self.info_file_name), \n",
    "                    info_set='info', set_identifier=identifier, name='start', \n",
    "                    timestamp=h5py_funcs.parameterstudy_using_info_file._current_datetime_as_string())\n",
    "    self.lock_info_file.release()\n",
    "\n",
    "    samples = {}\n",
    "    is_not_first_runread = False\n",
    "    num_runs = problem['num_runs']\n",
    "    for run in range(num_runs):\n",
    "        print(print_prefix + 'start run {} for problem {}'.format(run, problem))\n",
    "        for read in num_reads:\n",
    "            _ans = composite.sample_qubo(**problem['kwargs_sampling'].update({'num_reads': read}))\n",
    "            _ans = _ans.to_serializable(pack_samples=False)\n",
    "            if is_not_first_runread:\n",
    "                _ans.pop('embedding_context')\n",
    "            else:\n",
    "                pass\n",
    "            samples.update({'{:0{width}n}_{:0{width2}n}'.format(run,read,width=len(str(num_runs)),width2=len(str(num_reads))) : _ans})\n",
    "            is_not_first_runread = True\n",
    "        print(print_prefix + 'finished run {} for problem {}'.format(run, problem))\n",
    "    \n",
    "    self.lock_info_file.acquire()\n",
    "    h5py_funcs.parameterstudy_using_info_file.update_timestamp_in_info_file(file_name_path=os.path.join(self.folder_path_main, self.info_file_name), \n",
    "                    info_set='info', set_identifier=identifier, name='finish', \n",
    "                    timestamp=h5py_funcs.parameterstudy_using_info_file._current_datetime_as_string())\n",
    "    self.lock_info_file.release()\n",
    "\n",
    "    dict_data.update({'samples': samples})\n",
    "\n",
    "    kwargs_write_answer = {'file_name_path': os.path.join(self.folder_path_main, 'samples', identifier+'.h5'),\n",
    "                            'dict_data': dict_data.copy(),\n",
    "                            'data_name': 'sampleset',\n",
    "                            'name_suffix': '', \n",
    "                            'overwrite_data_in_file': False,\n",
    "                            'track_order': True}\n",
    "    print(print_prefix + 'succesfully retrieved answer for problem {}', problem)\n",
    "    return kwargs_write_answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overloaded_writer_work(self, answer, verbose=0, print_prefix=''):\n",
    "    print(print_prefix + 'start writing answer for problem {}', answer['dict_data']['set_identifier'])\n",
    "    h5py_funcs.io.write_to_hdf5_file(**answer)\n",
    "    print(print_prefix + 'finished writing answer for problem {}', answer['dict_data']['set_identifier'])\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def child_process_target(**kwargs):\n",
    "    cpn = multiprocessing.current_process().name\n",
    "    print(print_prefix + cpn, 'started with inputs', kwargs['target'])\n",
    "    #answer = overloaded_submitter_work(*kwargs['submitter']['args'], **kwargs['submitter']['kwargs'])\n",
    "    #overloaded_writer_work(*((answer,) + kwargs['writer']['args']), **kwargs['writer']['kwargs'])\n",
    "\n",
    "    st = leap_funcs.qubo.parameterstudy.Multithread_Variationstudy(\n",
    "        num_threads_submitters=5, num_threads_writers=5\n",
    "    )\n",
    "    st.submitter_work = overloaded_submitter_work\n",
    "    st.writer_work = overloaded_writer_work\n",
    "    st.folder_path_main = kwargs['folder_path_main']\n",
    "    st.info_file_name = kwargs['info_file_name']\n",
    "    st.problems = kwargs['submitter']['args'][0]\n",
    "    st.solver.update(**kwargs['target']['kwargs_dwavesampler'])\n",
    "    st.start_execution(verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _main_create_chunks(arg={}, num_chunks=0):\n",
    "    splitter = np.array_split(list(arg.keys()), num_chunks)\n",
    "    chunks = [arg[spl[i]] for spl in splitter for i in range(len(spl))]\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the main process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_chunks = 1\n",
      "__main__\n",
      "chunk 1 of 1\n",
      "chunk 2 of 1\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    folder_path_main = os.path.join(os.getcwd(), '01_out', 'sub_1')\n",
    "    info_file_name = 'study_nearest_neighbours_info.h5'\n",
    "    #_main_create_info_file(folder_path_main=folder_path_main, info_file_name=info_file_name)\n",
    "    reread_info_file = read_info_file(os.path.join(folder_path_main, info_file_name), infoset_name='')\n",
    "    \n",
    "    #_main_find_embedding(reread_info_file=reread_info_file, folder_path_main=folder_path_main)\n",
    "    reread_embeddings = read_embeddings(reread_info_file=reread_info_file, folder_path_main=folder_path_main)\n",
    "    sampler_params = {id.decode('utf-8'): \n",
    "                        {'embedding': list(reread_embeddings.keys())[i],\n",
    "                         'num_runs': 10,\n",
    "                         'kwargs_sampling': {'num_reads':1000}\n",
    "                         }\n",
    "                    for i, id in enumerate(reread_info_file['info']['study']['data']['identifiers'][:len(reread_embeddings)])}\n",
    "    \n",
    "    iterations = len(sampler_params)\n",
    "    chunk_size = 50\n",
    "    num_chunks = np.ceil(iterations/chunk_size).astype(int)\n",
    "    chunks = _main_create_chunks(sampler_params, num_chunks)\n",
    "\n",
    "    multiprocessing.set_start_method('spawn')\n",
    "    print('num_chunks =', num_chunks)\n",
    "    print(__name__)\n",
    "    \n",
    "    with open('../API_Token_Dev.txt', mode='rt') as file:\n",
    "        token = file.readline().rstrip()\n",
    "        #kwargs_dwavesampler = {'token': token, 'architecture': 'pegasus', 'region': 'eu-central-1'}\n",
    "\n",
    "    for chunk_id, chunk in enumerate(chunks[0:2]):\n",
    "        print('chunk', chunk_id+1, 'of', num_chunks)\n",
    "        #p = multiprocessing.Process(target=test_function.f_test)\n",
    "        kwargs_dwavesampler = {'token' : token, 'region':'eu-central-1', 'architecture':'pegasus', 'name':'Advantage_system5.4'}\n",
    "        kwargs_target = {'print_prefix': ' ', 'kwargs_dwavesampler': kwargs_dwavesampler, 'folder_path_main': folder_path_main, 'info_file_name': info_file_name}\n",
    "        inputs_submitter = {'args': chunk, 'kwargs':{'print_prefix': ' '}}\n",
    "        inputs_writer = {'args': (), 'kwargs':{'print_prefix': ' '}}\n",
    "        inputs_target = {'args': (), 'kwargs':{'target': kwargs_target, 'submitter': inputs_submitter, 'writer': inputs_writer}}\n",
    "\n",
    "        p = multiprocessing.Process(target=child_process_target, args=inputs_target['args'], kwargs=inputs_target['kwargs'])\n",
    "        p.start()\n",
    "        p.join()\n",
    "        p.close()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return sampler_params\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print('This is the main process')\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterth_file",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
