{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_section(text):\n",
    "    print('##############################################')\n",
    "    print('###  ', text)\n",
    "    print('##############################################')\n",
    "\n",
    "\n",
    "print_section('Start imports')\n",
    "import importlib\n",
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "sys.path.append(str(pathlib.PurePath(pathlib.Path.cwd().parent)))\n",
    "\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.optimize\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.axes as am\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget\n",
    "import statsmodels.api as sm\n",
    "import SALib\n",
    "import SALib.analyze.sobol\n",
    "import itertools\n",
    "\n",
    "import dimod\n",
    "import dwave\n",
    "import dwave.system\n",
    "from dwave.system import DWaveSampler, EmbeddingComposite, FixedEmbeddingComposite\n",
    "import dwave.inspector\n",
    "import dwave_networkx as dnx\n",
    "import minorminer\n",
    "\n",
    "from src.particle_funcs import distance_matrix as distance_matrix\n",
    "from src.particle_funcs import io as particles_io\n",
    "import src.leap_funcs.qubo.q_matrix as q_matrix\n",
    "\n",
    "from src import leap_funcs as leap_funcs\n",
    "from src.leap_funcs import embedding_quality\n",
    "from src.leap_funcs.qubo import parameterstudy\n",
    "\n",
    "from src import h5py_funcs\n",
    "from src.h5py_funcs import inspections, discoveries, init_custom_getstates, io, parameterstudy_using_info_file\n",
    "\n",
    "print_section('Finished imports')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "print('io.DEFAULT_BUFFER_SIZE', io.DEFAULT_BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_from = 'pickle'  # 'pickle' only, 'marshal' unmarshals numpy.ndarray as bytes-object and raises error when marshaling pathlib.Path objects\n",
    "assert read_from == 'pickle', 'marshal unmarshals numpy.ndarray as bytes-object and raises error when marshaling pathlib.Path objects'\n",
    "deserializer = ...\n",
    "if read_from == 'pickle':\n",
    "    deserializer = pickle\n",
    "elif read_from == 'marshal':\n",
    "    deserializer = marshal\n",
    "else:\n",
    "    raise ValueError(f'Unknown read_from value: {read_from}. Use \"pickle\" or \"marshal\".')\n",
    "\n",
    "data_dir = pathlib.Path('03_inspect/01_data')\n",
    "\n",
    "dict_files_information, dict_infos_read, dict_study, dict_success = \\\n",
    "{}, {}, {}, {}\n",
    "\n",
    "files_to_read = {'dict_files_information': {'is_to_load': True },\n",
    "                 'dict_infos_read'       : {'is_to_load': True },\n",
    "                 'dict_study': {\n",
    "                    1: {'is_to_load': True },\n",
    "                    2: {'is_to_load': True },\n",
    "                    3: {'is_to_load': True },\n",
    "                    4: {'is_to_load': True },\n",
    "                    5: {'is_to_load': True },\n",
    "                    6: {'is_to_load': True },\n",
    "                    7: {'is_to_load': True },\n",
    "                    8: {'is_to_load': True }\n",
    "                    },\n",
    "                 'dict_success': {\n",
    "                    1: {'is_to_load': False},\n",
    "                    2: {'is_to_load': False},\n",
    "                    3: {'is_to_load': False},\n",
    "                    4: {'is_to_load': False},\n",
    "                    5: {'is_to_load': False},\n",
    "                    6: {'is_to_load': False},\n",
    "                    7: {'is_to_load': False},\n",
    "                    8: {'is_to_load': False}\n",
    "                    }\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in files_to_read.items():\n",
    "    if key in ('dict_files_information', 'dict_infos_read'):\n",
    "        exec(f'{key} = None')  # create variables dynamically\n",
    "        if val['is_to_load']:\n",
    "            file_name_path = data_dir.joinpath(key + '.' + 'pickle')\n",
    "            print(f'Reading {key} from {file_name_path}')\n",
    "            with open(file_name_path, 'rb') as f:\n",
    "                exec(f'{key} = deserializer.load(f)')\n",
    "    elif key in ('dict_study', 'dict_success'):\n",
    "        exec(f'{key} = dict()')  # create variables dynamically\n",
    "        for sub_key, sub_val in val.items():\n",
    "            if sub_val['is_to_load']:\n",
    "                file_name_path = data_dir.joinpath(f'{key}_{sub_key}'+ '.' + 'pickle')\n",
    "                print(f'Reading {key}[{sub_key}] from {file_name_path}')\n",
    "                with open(file_name_path, 'rb') as f:\n",
    "                    exec(f'{key}[{sub_key}] = deserializer.load(f)')\n",
    "            else:\n",
    "                exec(f'{key}[{sub_key}] = None')\n",
    "    else:\n",
    "        raise ValueError(f'Unknown key: {key}. Check files_to_read')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sim_annealing_solution(study:int|None=None, substudy:int|None=None, num_particles:int=None, num_neighbours:int|None=None, qubos:dict|None=None):\n",
    "    print_section('Obtain exact/correct solution via simulated annealing - Start')\n",
    "    import ast\n",
    "    import dwave.samplers\n",
    "    if num_neighbours is None:\n",
    "        num_neighbours = num_particles\n",
    "    #num_particles = 5\n",
    "    qubos_key = f'{num_particles}_{num_neighbours}'\n",
    "    qubos_key_long = f'0{num_particles}_0{num_neighbours}'\n",
    "    qubos_key_long_short = f'0{num_particles}_{num_neighbours}'\n",
    "    qubos_key_short_long = f'{num_particles}_0{num_neighbours}'\n",
    "    if qubos is None:\n",
    "        _qubos = dict_infos_read[study][substudy]['qubos']\n",
    "    else:\n",
    "        _qubos = qubos\n",
    "    if qubos_key in _qubos:\n",
    "        #qubo = dict_infos_read[study][substudy]['qubos'][f'{num_particles}_{num_particles}']\n",
    "        qubo = _qubos[qubos_key]\n",
    "    elif qubos_key_long in _qubos:\n",
    "        qubo = _qubos[qubos_key_long]\n",
    "    elif qubos_key_long_short in _qubos:\n",
    "        qubo = _qubos[qubos_key_long_short]\n",
    "    elif qubos_key_short_long in _qubos:\n",
    "        qubo = _qubos[qubos_key_short_long]\n",
    "    else:\n",
    "        raise ValueError(f'QUBO key {qubos_key} not found in provided qubo dict. Available keys: {list(_qubos.keys())}')\n",
    "\n",
    "    #sim_annealing_sample = dimod.samplers.ExactSolver().sample_qubo(\n",
    "    #    {ast.literal_eval(key): value['data'] for key, value in qubo.items()})\n",
    "    sim_annealing_sample = dwave.samplers.SimulatedAnnealingSampler().sample_qubo(\n",
    "        {ast.literal_eval(key): value['data'] for key, value in qubo.items()},\n",
    "        num_reads=10000)\n",
    "    sim_annealing_sample = sim_annealing_sample.aggregate() # accumulates number of occurences\n",
    "    #print(type(sim_annealing_sample))\n",
    "    exact_sol = sim_annealing_sample.record\n",
    "    exact_sol.sort(order='energy')\n",
    "    #print(exact_sol)\n",
    "    print_section('Obtain correct solution via simulated annealing - Finished')\n",
    "    return exact_sol\n",
    "print(compute_sim_annealing_solution(study=2, substudy=0, num_particles=5, num_neighbours=5))\n",
    "print(compute_sim_annealing_solution(qubos=dict_infos_read[1][0]['qubos'], num_particles=5, num_neighbours=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, v in enumerate(dict_study[2]):\n",
    "    for k2, v2 in v.items():\n",
    "        print(i, k2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for study_key in dict_study.keys():\n",
    "    # if study_key in [0,1,2]:\n",
    "    #     continue\n",
    "    if dict_study[study_key] is None:\n",
    "        continue\n",
    "    for i, v in enumerate(dict_study[study_key]):\n",
    "        print(study_key, i)\n",
    "        for k2, v2 in v.items():\n",
    "            #print(i, k2, list(v2.keys()), list(v2['custom'].keys()))\n",
    "            dict_study[study_key][i][k2] = v2['custom']\n",
    "            #del v2['custom']\n",
    "# for k2, v2 in dict_study[2][0].items():\n",
    "#     print(i, k2, list(v2.keys()), list(v2['custom'].keys()))\n",
    "#     dict_study[2][0][k2] = v2['custom']\n",
    "    #del v2['custom']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, v in enumerate(dict_study[2]):\n",
    "    for k2, v2 in v.items():\n",
    "        print(i, k2)\n",
    "        for k3, v3 in v2.items():\n",
    "            print(' ', k3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(inspections)\n",
    "importlib.reload(inspections.stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(dict_study[2][0], orient='index').reset_index(names=['set_id'])\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_params_from_info = pd.DataFrame.from_records(dict_infos_read[2][0]['study']['data']['sets'][:,0], index=dict_infos_read[2][0]['study']['data']['identifiers'])\n",
    "dict_df_params_from_info = {}\n",
    "dict_substudies_need_merge = {}\n",
    "for key, val in dict_study.items():\n",
    "    dict_df_params_from_info[key] = None\n",
    "    dict_substudies_need_merge[key] = None\n",
    "    print(key)\n",
    "    if val is not None:\n",
    "        dict_df_params_from_info[key] = []\n",
    "        dict_substudies_need_merge[key] = [[0]]\n",
    "        study_id = key\n",
    "        for substudy_id in range(len(dict_infos_read[study_id])):\n",
    "            dict_df_params_from_info[key].append(None)\n",
    "            study_info = dict_infos_read[study_id][substudy_id]['study']['data']\n",
    "            study_info_data_sets = dict_infos_read[study_id][substudy_id]['study']['data']['sets']\n",
    "            dtype_names = study_info_data_sets.dtype.names\n",
    "            print(dtype_names)\n",
    "            if study_info_data_sets.ndim == 1:\n",
    "                study_info_data_sets = np.atleast_2d(study_info_data_sets).T # Transpose because np.atleast_2d does (n,) -> (1, n), but we need (n, 1) to be consistent\n",
    "            dict_df_params_from_info[study_id][substudy_id] = pd.DataFrame(dict_infos_read[study_id][substudy_id]['study']['data']['identifiers'], columns=['identifiers']).merge(\n",
    "                            pd.DataFrame(study_info_data_sets[:,0]), left_index=True, right_index=True)\n",
    "            dict_df_params_from_info[study_id][substudy_id]['identifiers'] = dict_df_params_from_info[study_id][substudy_id]['identifiers'].apply(lambda id: id.decode('utf-8'))\n",
    "            _shape = dict_df_params_from_info[study_id][substudy_id].shape\n",
    "            if substudy_id > 0:\n",
    "                _is_equal_to_previous = dict_df_params_from_info[study_id][substudy_id].equals(dict_df_params_from_info[study_id][substudy_id-1])\n",
    "                if _is_equal_to_previous:\n",
    "                    dict_substudies_need_merge[study_id][-1].append(substudy_id)\n",
    "                else:\n",
    "                    dict_substudies_need_merge[study_id].append([substudy_id])\n",
    "                print(_shape, end=' ')\n",
    "                print('  is equal to previous substudy:', _is_equal_to_previous)\n",
    "            else:\n",
    "                print(_shape)\n",
    "dict_substudies_need_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dict_study_merged_substudies = {}\n",
    "for key, val in dict_study.items():\n",
    "    print(key)\n",
    "    dict_study_merged_substudies[key] = []\n",
    "    if val is not None:\n",
    "        if len(dict_substudies_need_merge[key]) == 0:\n",
    "            print(f'Study {key} does not require merge.')\n",
    "            dict_study_merged_substudies[key].append(dict_study[key][0])\n",
    "            continue\n",
    "\n",
    "        for ids_to_merge in dict_substudies_need_merge[key]:\n",
    "            merged_ids = []\n",
    "            merged_dict = {}\n",
    "            print(f'IDs to merge: {ids_to_merge}')\n",
    "            for id in ids_to_merge:\n",
    "                print(f'ID {id} not merged, merge now to', merged_ids)\n",
    "                for key_dict_1 in merged_dict.keys():\n",
    "                    if key_dict_1 in list(dict_study[key][id].keys()):\n",
    "                        raise ValueError(f'Problem merging substudy {id} because {key_dict_1} already exists in merged_dict.')\n",
    "                merged_dict.update(dict_study[key][id])\n",
    "                merged_ids.append(id)\n",
    "            dict_study_merged_substudies[key].append(merged_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dict_study_merged_substudies.keys():\n",
    "    print(len(dict_study_merged_substudies[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_success_2_local = inspections.extract_success_dict(dict_for_df=dict_study[2][0],\n",
    "                                                        exact_sols=compute_sim_annealing_solution(study=2, substudy=0, num_particles=5),\n",
    "                                                        n_samples_to_compare=10, n_exact_sols_to_compare=10, is_skip_custom_key_in_dict_for_df=True)\n",
    "list(dict_success_2_local.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_cache_dict_sim_anneling_solutions = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_success_dict_study_1(dict_for_df:dict=None, n_samples_to_compare:int=0, n_exact_sols_to_compare:int=0,\n",
    "                         is_skip_custom_key_in_dict_for_df:bool=False, is_print_sols:bool=False, is_print_meta:bool=False, print_prefix:str=' '):\n",
    "    global _cache_dict_sim_anneling_solutions\n",
    "    dict_success_dicts = {}\n",
    "    print(f'Extract {dict_infos_read[1][0]['study']['data'].shape[0]} success dicts from study 1...')\n",
    "    for row_id in range(dict_infos_read[1][0]['study']['data'].shape[0]):\n",
    "        _num_particels = dict_infos_read[1][0]['study']['data']['sets'][row_id]['num_particles'][0]\n",
    "        _num_neighbours = dict_infos_read[1][0]['study']['data']['sets'][row_id]['num_nearest_neighbours'][0]\n",
    "        _id = dict_infos_read[1][0]['study']['data']['identifiers'][row_id]\n",
    "        print(f'  Processing num_particles={_num_particels}, num_neighbours={_num_neighbours} for id={_id}')\n",
    "        #if _num_particels>=6: \n",
    "        #    continue\n",
    "        if row_id in _cache_dict_sim_anneling_solutions[1].keys():\n",
    "            print(f'  Found cached solution for id={_id} (row_id: {row_id})')\n",
    "        else:\n",
    "            _cache_dict_sim_anneling_solutions[1][row_id] = compute_sim_annealing_solution(qubos=dict_infos_read[1][0]['qubos'], num_particles=_num_particels, num_neighbours=_num_neighbours)\n",
    "        \n",
    "        #dict_success_dicts[_id.decode('utf-8')] = inspections.extract_success_dict(dict_for_df=dict_for_df,\n",
    "        _id_decoded = _id.decode('utf-8')\n",
    "        dict_success_dicts.update( inspections.extract_success_dict(dict_for_df={_id_decoded:dict_for_df[_id_decoded]},\n",
    "                                                             exact_sols=_cache_dict_sim_anneling_solutions[1][row_id],\n",
    "                                                             n_samples_to_compare=n_samples_to_compare,\n",
    "                                                             n_exact_sols_to_compare=n_exact_sols_to_compare,\n",
    "                                                             is_skip_custom_key_in_dict_for_df=is_skip_custom_key_in_dict_for_df,\n",
    "                                                             is_print_sols=is_print_sols, is_print_meta=is_print_meta, print_prefix=print_prefix + ' '))\n",
    "    return dict_success_dicts    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples_data = dict_study_merged_substudies[2][0]['zz_4593727025']['sampleset']['000_0142']['_record']['data']\n",
    "# list_dfs = []\n",
    "# for run_sub_key, run_sub in  dict_study_merged_substudies[2][0]['zz_4593727025']['sampleset'].items():\n",
    "#     list_dfs.append(pd.DataFrame(run_sub['_record']['data'].tolist(), columns=run_sub['_record']['data'].dtype.names))\n",
    "# combined_df = pd.concat(list_dfs)\n",
    "# combined_df['sample'] = combined_df['sample'].apply(lambda x: tuple(x))\n",
    "# res_df = combined_df.groupby('sample', as_index=False).aggregate({'energy':'first',\t'num_occurrences':'sum','chain_break_fraction': 'first'})\n",
    "# res_df.sort_values('energy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_success_local_from_merged_studies = {}\n",
    "#n_samples_to_compare = (1,2,3,4)\n",
    "#n_exact_sols_to_compare = (1,2,3,4)\n",
    "n_samples_to_compare = (4,)\n",
    "n_exact_sols_to_compare = (4,)\n",
    "is_skip_custom_key_in_dict_for_df = True\n",
    "for study_key in dict_study_merged_substudies.keys():\n",
    "    #if  study_key !=5: continue\n",
    "    if study_key in _cache_dict_sim_anneling_solutions.keys():\n",
    "        pass\n",
    "    else:\n",
    "        _cache_dict_sim_anneling_solutions[study_key] = {}\n",
    "    dict_success_local_from_merged_studies[study_key] = {}\n",
    "    print(f'Processing study {study_key}...')\n",
    "    if dict_study_merged_substudies[study_key] is not None and study_key != 1:\n",
    "        for merged_substudies_key in range(len(dict_study_merged_substudies[study_key])):\n",
    "            print(f'  Processing merged substudies {merged_substudies_key}...i.e. substudies {dict_substudies_need_merge[study_key][merged_substudies_key]}')\n",
    "            if merged_substudies_key in _cache_dict_sim_anneling_solutions[study_key].keys():\n",
    "                pass\n",
    "            else:\n",
    "                _cache_dict_sim_anneling_solutions[study_key][merged_substudies_key] = compute_sim_annealing_solution(study=study_key, substudy=dict_substudies_need_merge[study_key][merged_substudies_key][0], num_particles=5)\n",
    "            \n",
    "            for i, (_n_sam, _n_ex) in enumerate(itertools.product(n_samples_to_compare, n_exact_sols_to_compare)):\n",
    "                if _n_sam > _n_ex:\n",
    "                    continue\n",
    "\n",
    "                print(f'  Comparing samples: {_n_sam} with exact solutions: {_n_ex}')\n",
    "                _kwargs = {'dict_for_df':dict_study_merged_substudies[study_key][merged_substudies_key],\n",
    "                #_kwargs = {'dict_for_df':{k: v for k,v in dict_study_merged_substudies[study_key][merged_substudies_key].items() \n",
    "                #                          if k == list(dict_study_merged_substudies[study_key][merged_substudies_key].keys())[0]\n",
    "                #                          or k == list(dict_study_merged_substudies[study_key][merged_substudies_key].keys())[1]\n",
    "                #                          or k == list(dict_study_merged_substudies[study_key][merged_substudies_key].keys())[2]},\n",
    "                            'exact_sols':_cache_dict_sim_anneling_solutions[study_key][merged_substudies_key],\n",
    "                            'n_samples_to_compare':_n_sam, 'n_exact_sols_to_compare':_n_ex, \n",
    "                            'is_skip_custom_key_in_dict_for_df':is_skip_custom_key_in_dict_for_df}\n",
    "                \n",
    "                if i == 0:\n",
    "                    #dict_success_local_from_merged_studies[study_key][merged_substudies_key] = inspections.extract_success_dict(**_kwargs)\n",
    "                    #dict_success_local_from_merged_studies[study_key][merged_substudies_key] = [inspections.extract_success_dict(**_kwargs)]\n",
    "                    dict_success_local_from_merged_studies[study_key][merged_substudies_key] = inspections.extract_success_dict(**_kwargs)\n",
    "                else:\n",
    "                    assert False\n",
    "                    for row_id, val in inspections.extract_success_dict(**_kwargs).items():\n",
    "                        for ee in [e for e in list(val.keys()) if e.startswith('fraction_samples_')]:\n",
    "                            print('      ', row_id, ee, val[ee])\n",
    "                        #dict_success_local_from_merged_studies[study_key][merged_substudies_key][row_id].update({k:v for k,v in val.items() \n",
    "                        #                                                                             if k not in dict_success_local_from_merged_studies[study_key][merged_substudies_key][row_id]})\n",
    "                    dict_success_local_from_merged_studies[study_key][merged_substudies_key].append(inspections.extract_success_dict(**_kwargs))\n",
    "\n",
    "    elif dict_study_merged_substudies[study_key] is not None and study_key == 1:\n",
    "        \n",
    "        for merged_substudies_key in range(len(dict_study_merged_substudies[study_key])):\n",
    "            dict_success_local_from_merged_studies[study_key][merged_substudies_key] = {}\n",
    "            \n",
    "            for i, (_n_sam, _n_ex) in enumerate(itertools.product(n_samples_to_compare, n_exact_sols_to_compare)):\n",
    "                if _n_sam > _n_ex:\n",
    "                    continue\n",
    "\n",
    "                print(f'  Comparing samples: {_n_sam} with exact solutions: {_n_ex}')\n",
    "                _kwargs = {'dict_for_df':dict_study_merged_substudies[study_key][0], \n",
    "                           'n_samples_to_compare':_n_sam,\n",
    "                           'n_exact_sols_to_compare':_n_ex,\n",
    "                           'is_skip_custom_key_in_dict_for_df':is_skip_custom_key_in_dict_for_df}\n",
    "                if i == 0:\n",
    "                    dict_success_local_from_merged_studies[study_key][merged_substudies_key] = extract_success_dict_study_1(**_kwargs)\n",
    "                else:\n",
    "                    for row_id, val in extract_success_dict_study_1(**_kwargs).items():\n",
    "                        dict_success_local_from_merged_studies[study_key][merged_substudies_key][row_id].update({k:v for k,v in val.items() \n",
    "                                                                                                     if k not in dict_success_local_from_merged_studies[study_key][merged_substudies_key][row_id]})\n",
    "    else:\n",
    "        dict_success_local_from_merged_studies[study_key] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(val['num_samples_matched_corrected_1_2'] for val in dict_success_local_from_merged_studies[5][0].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in dict_success_local_from_merged_studies[5][0].items():\n",
    "    print(k)\n",
    "    for kk, vv in v.items():\n",
    "        if kk=='num_samples' or kk=='fraction_samples_is_found_best' or kk.startswith('num_samples_matched_corrected_') or kk.startswith('fraction_samples_matched_corrected_'):\n",
    "            print('...', kk, vv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ids = {}\n",
    "#for k, v in dict_success_local_from_merged_studies[2][0][4].items():\n",
    "for k, v in dict_success_local_from_merged_studies[2][0].items():\n",
    "    print(k)\n",
    "    for kk, vv in v.items():\n",
    "        if kk=='num_samples' or kk=='fraction_samples_is_found_best' or kk.startswith('num_samples_matched_corrected_') or kk.startswith('fraction_samples_matched_corrected_'):\n",
    "            print('...', kk, vv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_success_local_from_merged_studies = {}\n",
    "# n_samples_to_compare = 10\n",
    "# n_exact_sols_to_compare = 10\n",
    "# is_skip_custom_key_in_dict_for_df = True\n",
    "# for study_key in dict_study_merged_substudies.keys():\n",
    "#     dict_success_local_from_merged_studies[study_key] = {}\n",
    "#     print(f'Processing study {study_key}...')\n",
    "#     if dict_study_merged_substudies[study_key] is not None and study_key != 1:\n",
    "#         for merged_substudies_key in range(len(dict_study_merged_substudies[study_key])):\n",
    "#             print(f'  Processing merged substudies {merged_substudies_key}...i.e. substudies {dict_substudies_need_merge[study_key][merged_substudies_key]}')\n",
    "#             dict_success_local_from_merged_studies[study_key][merged_substudies_key] = inspections.extract_success_dict(dict_for_df=dict_study_merged_substudies[study_key][merged_substudies_key],\n",
    "#                                                         exact_sols=compute_sim_annealing_solution(study=study_key, substudy=dict_substudies_need_merge[study_key][merged_substudies_key][0], num_particles=5),\n",
    "#                                                         n_samples_to_compare=n_samples_to_compare, n_exact_sols_to_compare=n_exact_sols_to_compare, is_skip_custom_key_in_dict_for_df=is_skip_custom_key_in_dict_for_df)\n",
    "#     elif dict_study_merged_substudies[study_key] is not None and study_key == 1:\n",
    "        \n",
    "#         for merged_substudies_key in range(len(dict_study_merged_substudies[study_key])):\n",
    "#             dict_success_local_from_merged_studies[study_key][merged_substudies_key] = {}\n",
    "#             dict_success_local_from_merged_studies[study_key][merged_substudies_key] = extract_success_dict_study_1(dict_for_df=dict_study_merged_substudies[study_key][0], n_samples_to_compare=n_samples_to_compare, \n",
    "#                                                                                 n_exact_sols_to_compare=n_exact_sols_to_compare, is_skip_custom_key_in_dict_for_df=is_skip_custom_key_in_dict_for_df)\n",
    "#     else:\n",
    "#         dict_success_local_from_merged_studies[study_key] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(dict_success_local_from_merged_studies[1][0].keys()))\n",
    "print(list(dict_success_local_from_merged_studies[1][0]['zz_5703083384'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(dict_study_merged_substudies[3][0].keys()))\n",
    "print(dict_success_local_from_merged_studies[3][0]['zz_0594500236'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dict_success_local_from_merged_studies[3][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_success_2_local = pd.DataFrame.from_dict(dict_success_2_local, orient='index').reset_index(names=['set_id'])\n",
    "print(df_success_2_local.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_success_2_merged_local = pd.DataFrame.from_dict(dict_success_local_from_merged_studies[3][0], orient='index').reset_index(names=['set_id'])\n",
    "print(df_success_2_merged_local.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_df_success_local =  {}\n",
    "# for study_key in dict_study.keys():\n",
    "#     dict_df_success_local[study_key] = {}\n",
    "#     print(f'Processing study {study_key}...')\n",
    "#     if dict_success_local[study_key] is not None:\n",
    "#         for substudy_key in range(len(dict_success_local[study_key])):\n",
    "#             print(f'  Processing substudy {substudy_key}...')\n",
    "#             dict_df_success_local[study_key][substudy_key] = pd.DataFrame.from_dict(dict_success_local[study_key][substudy_key], orient='index').reset_index(names=['set_id'])\n",
    "#             print(dict_df_success_local[study_key][substudy_key].head())\n",
    "#     else:\n",
    "#         dict_df_success_local[study_key] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_df_success_local_from_merged_studies =  {}\n",
    "for study_key in dict_study.keys():\n",
    "    dict_df_success_local_from_merged_studies[study_key] = {}\n",
    "    print(f'Processing study {study_key}...')\n",
    "    if dict_success_local_from_merged_studies[study_key] is not None:\n",
    "        for merged_substudies_key in range(len(dict_success_local_from_merged_studies[study_key])):\n",
    "            print(f'  Processing merged substudies {merged_substudies_key}...')\n",
    "            dict_df_success_local_from_merged_studies[study_key][merged_substudies_key] = pd.DataFrame.from_dict(dict_success_local_from_merged_studies[study_key][merged_substudies_key], orient='index').reset_index(names=['set_id'])\n",
    "            print(dict_df_success_local_from_merged_studies[study_key][merged_substudies_key].shape)\n",
    "    else:\n",
    "        dict_df_success_local_from_merged_studies[study_key] = None\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_df_success_local_from_merged_studies[5][0]['fraction_samples_is_found_best'].max()\n",
    "print(dict_df_success_local_from_merged_studies[5][0]['fraction_samples_matched_corrected_1_2'].idxmax())\n",
    "print(dict_df_success_local_from_merged_studies[5][0]['fraction_samples_matched_corrected_1_1'].idxmax())\n",
    "dict_df_success_local_from_merged_studies[5][0]['fraction_samples_matched_corrected_1_2'].iloc[335]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_params_success_2 = dict_df_params_from_info[2][0].merge(df_success_2_local, how='right', left_on='identifiers', right_on='set_id')\n",
    "print(df_merged_params_success_2.shape)\n",
    "df_merged_params_success_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict_df_merged_params_success= {}\n",
    "##dict_df_success_local =  {}\n",
    "#for study_key in dict_df_success_local.keys():\n",
    "#    dict_df_merged_params_success[study_key] = {}\n",
    "#    print(f'Processing study {study_key}...')\n",
    "#    if dict_df_success_local[study_key] is not None:\n",
    "#        for substudy_key in range(len(dict_df_success_local[study_key])):\n",
    "#            print(f'  Processing substudy {substudy_key}...')\n",
    "#            dict_df_merged_params_success[study_key][substudy_key] = dict_df_params_from_info[study_key][0].merge(dict_df_success_local[study_key][substudy_key], how='right', left_on='identifiers', right_on='set_id')\n",
    "#            print(dict_df_merged_params_success[study_key][substudy_key].shape)\n",
    "#            #print(dict_df_merged_params_success[study_key][substudy_key].head())\n",
    "#    else:\n",
    "#        dict_df_merged_params_success[study_key] = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_df_params_from_info[2][0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_df_success_local_from_merged_studies[2][0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_df_success_local_from_merged_studies\n",
    "\n",
    "dict_df_merged_params_success_from_merged_studies= {}\n",
    "for study_key in dict_df_success_local_from_merged_studies.keys():\n",
    "    dict_df_merged_params_success_from_merged_studies[study_key] = {}\n",
    "    print(f'Processing study {study_key}...')\n",
    "    if dict_df_merged_params_success_from_merged_studies[study_key] is not None:\n",
    "        for merged_substudies_key in range(len(dict_df_success_local_from_merged_studies[study_key])):\n",
    "            print(f'  Processing substudy {merged_substudies_key}...')\n",
    "            dict_df_merged_params_success_from_merged_studies[study_key][merged_substudies_key] = dict_df_params_from_info[study_key][dict_substudies_need_merge[study_key][merged_substudies_key][0]].merge(dict_df_success_local_from_merged_studies[study_key][merged_substudies_key], how='right', left_on='identifiers', right_on='set_id')\n",
    "            print(dict_df_merged_params_success_from_merged_studies[study_key][merged_substudies_key].shape)\n",
    "            #print(dict_df_merged_params_success_from_merged_studies[study_key][merged_substudies_key].head())\n",
    "    else:\n",
    "        dict_df_merged_params_success_from_merged_studies[study_key] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_df_merged_params_success_from_merged_studies[7][0][['fraction_samples_is_found_best'] + [f'fraction_samples_matched_{i}_{j}' for i,j in itertools.product(n_samples_to_compare, n_exact_sols_to_compare) if j>=i]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_params_success_2_normalized = df_merged_params_success_2.copy()\n",
    "df_merged_params_success_2_normalized['annealing_time'] /= df_merged_params_success_2_normalized['annealing_time'].max()\n",
    "df_merged_params_success_2_normalized['programming_thermalization'] /= df_merged_params_success_2_normalized['programming_thermalization'].max()\n",
    "df_merged_params_success_2_normalized['readout_thermalization'] /= df_merged_params_success_2_normalized['readout_thermalization'].max()\n",
    "df_merged_params_success_2_normalized['estimated_runtime'] /= df_merged_params_success_2_normalized['estimated_runtime'].max()\n",
    "df_merged_params_success_2_normalized['fraction_samples_is_found_best'] /= df_merged_params_success_2_normalized['fraction_samples_is_found_best'].max()\n",
    "df_merged_params_success_2_normalized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_params_success_2_normalized.corr(method='pearson', numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_params_success_2_normalized.corr(method='kendall', numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_params_success_2_normalized.corr(method='spearman', numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_params_success_2_normalized.cov(numeric_only=True, ddof=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_params_success_2_normalized.var(numeric_only=True, ddof=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_params_success_2_normalized_only_numeric_cols = df_merged_params_success_2_normalized.select_dtypes(include=[np.number])\n",
    "df_merged_params_success_2_normalized_only_numeric_cols.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j, in zip(df_merged_params_success_2_normalized_only_numeric_cols.columns,\n",
    "                                   sm.stats.stattools.durbin_watson(df_merged_params_success_2_normalized_only_numeric_cols)):\n",
    "    print(f'{i}: {j}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j, in zip(df_merged_params_success_2_normalized_only_numeric_cols.columns,\n",
    "                                   sm.stats.stattools.jarque_bera(df_merged_params_success_2_normalized_only_numeric_cols)):\n",
    "    print(f'{i}: {j}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unistats= inspections.stats.univariate_statistics(df_merged_params_success_2_normalized_only_numeric_cols)\n",
    "unistats.compute_all_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(unistats.statistics.keys()))\n",
    "print(unistats.statistics['nth_moment_biased'][3])\n",
    "print(unistats.statistics['variance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.printoptions(precision=3, linewidth=200, edgeitems=3):\n",
    "    for id, col in enumerate(df_merged_params_success_2_normalized_only_numeric_cols.columns):\n",
    "        moments = np.array(tuple(unistats.statistics['nth_moment_biased'][nth][id] for nth in unistats.statistics['nth_moment_biased'].keys()))\n",
    "        lmoments = np.array(tuple(unistats.statistics['nth_lmoment'][nth][id] for nth in unistats.statistics['nth_lmoment'].keys()))\n",
    "        print(f'{col}:\\n', moments, '\\n', lmoments)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.printoptions(precision=3, linewidth=200, edgeitems=3):\n",
    "    for id, col in enumerate(df_merged_params_success_2_normalized_only_numeric_cols.columns):\n",
    "        gmeans = np.array(tuple(unistats.statistics['gmean']))\n",
    "        print(f'{col}:', len(gmeans), np.array([unistats.statistics['gmean'][id], unistats.statistics['hmean'][id]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.printoptions(precision=3, linewidth=200, edgeitems=3):\n",
    "    for id, col in enumerate(df_merged_params_success_2_normalized_only_numeric_cols.columns):\n",
    "        entropy = np.array(unistats.statistics['entropy'])\n",
    "        crossentropy = np.array(unistats.statistics['cross_entropy'][id])\n",
    "        print(f'{col}:', len(entropy), entropy[id], crossentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspections.stats.sp_stats.binned_statistic(df_merged_params_success_2['fraction_samples_is_found_best'], None, statistic='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspections.stats.sp_stats.binned_statistic_2d(x = df_merged_params_success_2['annealing_time'],\n",
    "                                               y = df_merged_params_success_2['readout_thermalization'], \n",
    "                                               values= df_merged_params_success_2['fraction_samples_is_found_best'],\n",
    "                                               bins=[10,10],\n",
    "                                               range=[[df_merged_params_success_2['annealing_time'].min(), df_merged_params_success_2['annealing_time'].max()],\n",
    "                                                       [df_merged_params_success_2['readout_thermalization'].min(), df_merged_params_success_2['readout_thermalization'].max()]],\n",
    "                                               statistic='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_df_merged_params_success[5][0].select_dtypes(include=[np.number]).to_numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspections.stats.sp_stats.binned_statistic_dd(sample = dict_df_merged_params_success[5][0].select_dtypes(include=[np.number]).to_numpy(),\n",
    "#                                                values = dict_df_merged_params_success[5][0]['fraction_samples_is_found_best'], \n",
    "#                                                #bins=[10,10],\n",
    "#                                                statistic='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_params_success_2[['annealing_time', 'readout_thermalization']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_salib_problem = {}\n",
    "for study_key in dict_df_merged_params_success_from_merged_studies.keys():\n",
    "    dict_salib_problem[study_key] = {}\n",
    "    print(f'Processing study {study_key}...')\n",
    "    if dict_df_merged_params_success_from_merged_studies[study_key] is not None and study_key > 1:\n",
    "        for merged_substudies_key in range(len(dict_df_merged_params_success_from_merged_studies[study_key])):\n",
    "            print(f'  Processing merged substudies {merged_substudies_key}...')\n",
    "            dict_salib_problem[study_key][merged_substudies_key] = None\n",
    "            _initial_substudy_key_for_info = dict_substudies_need_merge[study_key][merged_substudies_key][0]\n",
    "            if study_key not in (6, 7, 8):\n",
    "                dict_salib_problem[study_key][merged_substudies_key] = {'num_vars': dict_infos_read[study_key][_initial_substudy_key_for_info]['attrs']['num_vars'],\n",
    "                                                                        'names':    [n for n in dict_infos_read[study_key][_initial_substudy_key_for_info]['attrs']['names'] if n != 'estimated_runtime'],\n",
    "                                                                        'bounds':   dict_infos_read[study_key][_initial_substudy_key_for_info]['attrs']['bounds']\n",
    "                                                               }\n",
    "            else:\n",
    "                dict_salib_problem[study_key][merged_substudies_key] = {'num_vars': 0,\n",
    "                                                                        'names':    [n for n in dict_infos_read[study_key][_initial_substudy_key_for_info]['attrs']['names'] if\n",
    "                                                                            (n != 'estimated_runtime') and (n[0] not in ('t', 's'))],\n",
    "                                                                }\n",
    "                dict_salib_problem[study_key][merged_substudies_key]['names'].append('a')\n",
    "                dict_salib_problem[study_key][merged_substudies_key]['names'].append('b')\n",
    "                dict_salib_problem[study_key][merged_substudies_key]['num_vars'] = len(dict_salib_problem[study_key][merged_substudies_key]['names'])\n",
    "                dict_salib_problem[study_key][merged_substudies_key]['bounds'] = \\\n",
    "                                            dict_infos_read[study_key][_initial_substudy_key_for_info]['attrs']['bounds'][:-2+len(dict_salib_problem[study_key][merged_substudies_key]['names'])]\n",
    "                dict_salib_problem[study_key][merged_substudies_key]['bounds'] = np.append(dict_salib_problem[study_key][merged_substudies_key]['bounds'], np.array([[-1.0, 1.0],[0.0625, 1.0]]), axis=0)\n",
    "                                                               \n",
    "            assert dict_infos_read[study_key][_initial_substudy_key_for_info]['attrs']['names'][-1] == 'estimated_runtime'\n",
    "            for kkk, vvv in dict_salib_problem[study_key][merged_substudies_key].items():\n",
    "                print(f'    {kkk}: {vvv}')\n",
    "    #elif dict_df_success_local[study_key] is not None and study_key == 1:\n",
    "    elif dict_df_merged_params_success_from_merged_studies[study_key] is not None and study_key == 1:\n",
    "        print('There is only a single substudy.')\n",
    "        # dict_salib_problem[1][0] = {'num_vars': dict_infos_read[1][0]['attrs']['names'].shape[0],\n",
    "        #                             'names':    [dict_infos_read[1][0]['attrs']['names'][i] for i in range(dict_infos_read[1][0]['attrs']['names'].shape[0])],\n",
    "        #                             'bounds':   np.array([[dict_infos_read[1][0]['study']['data']['sets']['num_particles'].min(),\n",
    "        #                                                         dict_infos_read[1][0]['study']['data']['sets']['num_particles'].max()],\n",
    "        #                                                   [dict_infos_read[1][0]['study']['data']['sets']['num_nearest_neighbours'].min(),\n",
    "        #                                                         dict_infos_read[1][0]['study']['data']['sets']['num_nearest_neighbours'].max()]])}\n",
    "        dict_salib_problem[1][0] = {'num_vars': dict_infos_read[1][0]['attrs']['names'].shape[0],\n",
    "                                    'names':    [dict_infos_read[1][0]['attrs']['names'][0], f'Fraction_of_{dict_infos_read[1][0]['attrs']['names'][0]}'],\n",
    "                                    'bounds':   np.array([[dict_infos_read[1][0]['study']['data']['sets']['num_particles'].min(),\n",
    "                                                                dict_infos_read[1][0]['study']['data']['sets']['num_particles'].max()],\n",
    "                                                          [0.4, 1.0]])}\n",
    "        #print(dict_salib_problem[1][0])\n",
    "        for kkk, vvv in dict_salib_problem[1][0].items():\n",
    "                print(f'    {kkk}: {vvv}')\n",
    "    else:\n",
    "        dict_salib_problem[study_key] = None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def _curvefit_annealing_schedule(t, s):\n",
    "    \"\"\"\n",
    "    Fit a curve to the annealing schedule data.\n",
    "    cuve is of the form: a * exp(b * t) + c, with a {-1.0, 1.0}, b {0.0625, 1.0}, c =-a\n",
    "    :param t: time points {0.0, anneal_time}\n",
    "    :param s: anneal magnitude {0.0, 1.0}\n",
    "    :return: Fitted parameters\n",
    "    \"\"\"\n",
    "    def _func(x, a, b):\n",
    "        return a * np.exp(-b * x) - a\n",
    "    ab_array = np.zeros((t.shape[0], 2))\n",
    "    for i in range(t.shape[0]):\n",
    "        ab_array[i, :] = scipy.optimize.curve_fit(_func, t[i,:], s[i,:], bounds=([-1.0, 0.0625], [1.0, 1.0]))[0]\n",
    "    #print('Fitted parameters (a, b):', ab_array)\n",
    "    return ab_array\n",
    "    \n",
    "\n",
    "dict_salib_analyses = {}\n",
    "for study_key in dict_salib_problem.keys():\n",
    "    dict_salib_analyses[study_key] = []\n",
    "    print(f'Processing study {study_key}...')\n",
    "    if dict_df_success_local_from_merged_studies[study_key] is not None:# and study_key > 1:\n",
    "        for merged_substudies_key in range(len(dict_df_success_local_from_merged_studies[study_key])):\n",
    "            print(f'  Processing merged_substudies {merged_substudies_key}...i.e. substudies {dict_substudies_need_merge[study_key][merged_substudies_key]}')\n",
    "            _a = SALib.ProblemSpec(dict_salib_problem[study_key][merged_substudies_key])\n",
    "            #print(dict_infos_read[study_key][merged_substudies_key]['study']['data']['sets'])\n",
    "            _samples = dict_infos_read[study_key][merged_substudies_key]['study']['data']['sets']\n",
    "            if _samples.ndim == 1:\n",
    "                _samples = np.atleast_2d(_samples).T  # Transpose because np.atleast_2d does (n,) -> (1, n), but we need (n, 1) to be consistent\n",
    "            if study_key not in (1,6,7,8):\n",
    "                if study_key == 5 and merged_substudies_key in (0,1):\n",
    "                    _samples = _samples.view(np.float64)[:,:-1]\n",
    "                    _samples = np.pad(_samples, ((0, 2), (0, 0)), 'constant', constant_values=(0, 0))\n",
    "                    _a.set_samples(_samples)\n",
    "                else:\n",
    "                    _a.set_samples(_samples.view(np.float64)[:,:-1])  # Exclude the last two columns (a, b) for the analysis\n",
    "            elif study_key in (6,7,8):\n",
    "                print(_samples.view(np.float64).shape)\n",
    "                offset = 8 if study_key != 6 else 0\n",
    "                ab_array = _curvefit_annealing_schedule(_samples.view(np.float64)[:,offset:offset+12], _samples.view(np.float64)[:,offset+12:offset+12+12])  # Fit the annealing schedule curve\n",
    "                _col_id_to_insert = dict_df_merged_params_success_from_merged_studies[study_key][merged_substudies_key].columns.get_loc('t00')\n",
    "                dict_df_merged_params_success_from_merged_studies[study_key][merged_substudies_key].insert(_col_id_to_insert,     'a', ab_array[:, 0])\n",
    "                dict_df_merged_params_success_from_merged_studies[study_key][merged_substudies_key].insert(_col_id_to_insert + 1, 'b', ab_array[:, 1])\n",
    "                _samples = np.append(_samples.view(np.float64)[:,:_a['num_vars']-2],\n",
    "                                     ab_array, axis=1)  # Add two columns for a and b\n",
    "                _a.set_samples(_samples)  # Exclude the last two columns (a, b) for the analysis\n",
    "            elif study_key == 1:\n",
    "                _samples = _samples.view(np.int32)\n",
    "                _samples = np.pad(_samples, ((0, 1), (0, 0)), 'constant', constant_values=(0, 0))\n",
    "                print(_samples.view(np.int32).shape)\n",
    "                _a.set_samples(_samples.view(np.int32))\n",
    "\n",
    "            _Y = dict_df_success_local_from_merged_studies[study_key][merged_substudies_key]['fraction_samples_is_found_best'].to_numpy()\n",
    "            print(_a)\n",
    "            if _Y.shape[0] != _a.samples.shape[0]:\n",
    "                print(f'Padding _Y from {_Y.shape[0]} to {_a.samples.shape[0]}')\n",
    "                # Pad _Y with zeros to match the number of samples in _a\n",
    "                _Y = np.pad(_Y, (0, _a.samples.shape[0]-_Y.shape[0]), 'constant', constant_values=(0, 0))\n",
    "            _is_calc_second_order = True if _Y.shape[0] % (2*_a['num_vars']+2) == 0  else False\n",
    "            if study_key == 5 and merged_substudies_key in (0,1):\n",
    "                _is_calc_second_order = False\n",
    "            if study_key == 7:\n",
    "                print('  ', _Y.size % (2 * _a['num_vars'] + 2) == 0)\n",
    "                print('  ', _Y.size % (_a['num_vars'] + 2) == 0)\n",
    "                print(f'  _Y shape: {_Y.shape}, _Y.size: {_Y.size}, _a.num_vars: {_a['num_vars']}, _is_calc_second_order: {_is_calc_second_order}')\n",
    "\n",
    "            dict_salib_analyses[study_key].append(SALib.analyze.sobol.analyze(\n",
    "                problem=_a,\n",
    "                Y=_Y,\n",
    "                calc_second_order=_is_calc_second_order,\n",
    "                print_to_console=False\n",
    "            ))\n",
    "            print(dict_salib_analyses[study_key][merged_substudies_key])\n",
    "\n",
    "    elif dict_df_success_local_from_merged_studies[study_key] is not None and study_key == 1:\n",
    "        print('There is only a single substudy.')\n",
    "        # dict_salib_problem[1][0] = {'num_vars': dict_infos_read[1][0]['attrs']['names'].shape[0],\n",
    "        #                             'names':    dict_infos_read[1][0]['attrs']['names'],\n",
    "        #                             'bounds':   np.array([[dict_infos_read[1][0]['study']['data']['sets']['num_particles'].min(),\n",
    "        #                                                         dict_infos_read[1][0]['study']['data']['sets']['num_particles'].max()],\n",
    "        #                                                   [dict_infos_read[1][0]['study']['data']['sets']['num_nearest_neighbours'].min(),\n",
    "        #                                                         dict_infos_read[1][0]['study']['data']['sets']['num_nearest_neighbours'].max()]])}\n",
    "        dict_salib_problem[1][0] = {'num_vars': dict_infos_read[1][0]['attrs']['names'].shape[0],\n",
    "                                    'names':    [dict_infos_read[1][0]['attrs']['names'], 'Fraction_of_p1'],\n",
    "                                    'bounds':   np.array([[dict_infos_read[1][0]['study']['data']['sets']['num_particles'].min(),\n",
    "                                                                dict_infos_read[1][0]['study']['data']['sets']['num_particles'].max()],\n",
    "                                                          [0.4, 1.0]])}\n",
    "        print(dict_salib_problem[1][0])\n",
    "    else:\n",
    "        dict_salib_problem[study_key] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dict_salib_analyses.keys():\n",
    "    print(key)\n",
    "    if dict_salib_analyses[key] is not None:\n",
    "        print('  ', len(dict_salib_analyses[key]), [id for id in range(len(dict_salib_analyses[key]))])\n",
    "        for subkey in range(len(dict_salib_analyses[key])):\n",
    "            print('  ', key, subkey, dict_salib_problem[key][subkey]['num_vars'])\n",
    "            if dict_salib_problem[key][subkey]['num_vars']:\n",
    "                _plot = dict_salib_analyses[key][subkey].plot()\n",
    "                _plot[0].figure.set_size_inches(14, 10)\n",
    "                _plot[0].figure.suptitle('Sobol indices', fontsize=16)\n",
    "                _plot[0].set_title('Total Sobol indices', fontsize=14)\n",
    "                _plot[1].set_title('First order Sobol indices', fontsize=14)\n",
    "                if _plot.shape[0] == 3:\n",
    "                    _plot[2].set_title('Second order Sobol indices', fontsize=14)\n",
    "                for ax in _plot:\n",
    "                    ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "                    ax.tick_params(axis='both', which='minor', labelsize=12)\n",
    "                    ax.set_ylim((-1.0, 1.0))\n",
    "                plt.tight_layout()\n",
    "\n",
    "                plt.savefig(f'03_inspect/02_figs/salib_analysis_{key}_{subkey}.pdf')\n",
    "    else:\n",
    "        print('  No analyses available for this study.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_salib_analysis_7 = dict_salib_analyses[7][0].to_df()\n",
    "df_salib_analysis_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "gs = fig.add_gridspec(2, 2)\n",
    "axs0 = fig.add_subplot(gs[0, 0])\n",
    "axs1 = fig.add_subplot(gs[0, 1])\n",
    "axs2 = fig.add_subplot(gs[1, :])\n",
    "\n",
    "#axs[0, 0].bar(range(df_salib_analysis_7[1].shape[0]), df_salib_analysis_7[1]['S1'], label='First order Sobol indices', yerr=df_salib_analysis_7[1]['S1_conf'])\n",
    "df_salib_analysis_7[0].plot(kind='bar', ax=axs0, yerr='ST_conf')\n",
    "df_salib_analysis_7[1].plot(kind='bar', ax=axs1, yerr='S1_conf')\n",
    "df_salib_analysis_7[2].plot(kind='bar', ax=axs2, yerr='S2_conf')\n",
    "\n",
    "axs0.set_title('Total Sobol Indices')\n",
    "axs1.set_title('First Order Sobol Indices')\n",
    "axs2.set_title('Second Order Sobol Indices')\n",
    "\n",
    "for ax in fig.axes:\n",
    "    ax.grid(True, axis='y', linestyle='--', linewidth=0.25)\n",
    "#fig.suptitle('Sobol Indices for Study 7', fontsize=16)\n",
    "fig.tight_layout()\n",
    "plt.savefig('03_inspect/02_figs/salib_analysis_7_manual.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_salib_problem[7][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "dict_plots = {}\n",
    "bar_width = 0.1\n",
    "bar_spacing = 0.15\n",
    "for key in dict_salib_analyses.keys():\n",
    "    print(key)\n",
    "    dict_plots[key] = []\n",
    "    if dict_salib_analyses[key] is not None:\n",
    "        print('  ', key, len(dict_salib_analyses[key]), [id for id in range(len(dict_salib_analyses[key]))])\n",
    "        for subkey in range(len(dict_salib_analyses[key])):\n",
    "            print('    ', key, subkey, dict_salib_problem[key][subkey]['num_vars'])\n",
    "            _fig, _axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "            _num_vars = dict_salib_problem[key][subkey]['num_vars']\n",
    "            _axs[0].set_title(f'Sobol indices for study {key}, substudy {subkey}')\n",
    "            _axs[0].set_ylim(None, 1.0)\n",
    "            _axs[0].bar(x = np.arange(0, _num_vars*(bar_spacing), (bar_spacing)), height=dict_salib_analyses[key][subkey]['S1'], width=0.1, tick_label=list(dict_salib_problem[key][subkey]['names']))\n",
    "            _axs[0].set_xticklabels(_axs[0].get_xticklabels(),rotation=60, ha='right')\n",
    "            _fig.tight_layout()\n",
    "    else:\n",
    "        print('  No analyses available for this study.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "# Compare 2 & 3 & 4\n",
    "#####\n",
    "plt.close('all')\n",
    "\n",
    "x = np.arange(3)  # the label locations\n",
    "width = 0.25  # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "s2_ticklabels = [dict_salib_problem[2][0]['names'][0]+',\\n'+dict_salib_problem[2][0]['names'][1],\n",
    "                 dict_salib_problem[2][0]['names'][0]+',\\n'+dict_salib_problem[2][0]['names'][2],\n",
    "                 dict_salib_problem[2][0]['names'][1]+',\\n'+dict_salib_problem[2][0]['names'][2]]\n",
    "legend_labels = ['Study 2', 'Study 3 flux_drift_comp=false', 'Study 3 flux_drift_comp=true']\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3, layout='constrained', figsize=(12, 6))\n",
    "for i, id_name in enumerate(['ST', 'S1', 'S2']):\n",
    "#for i, id_name in enumerate(['S1']):\n",
    "    multiplier = 0\n",
    "    for study_id in range(2,5):\n",
    "        offset = width * multiplier\n",
    "        if id_name != 'S2':\n",
    "            #values = [dict_salib_analyses[2][0][id_name][id], dict_salib_analyses[3][0][id_name][id], dict_salib_analyses[4][0][id_name][id]]\n",
    "            #print(values)\n",
    "            values = dict_salib_analyses[study_id][0][id_name]\n",
    "            rects = axs[i].bar(x + offset, values, width, label=legend_labels[study_id-2])\n",
    "            axs[i].set_xticks(x + width, dict_salib_problem[2][0]['names'], rotation=90)\n",
    "        else:\n",
    "            if id == 0:\n",
    "                values = [dict_salib_analyses[study_id][0][id_name][0,1],\n",
    "                           dict_salib_analyses[study_id][0][id_name][0,2],\n",
    "                           dict_salib_analyses[study_id][0][id_name][1,2]]\n",
    "            elif id == 1:\n",
    "                values = [dict_salib_analyses[study_id][0][id_name][0,1],\n",
    "                           dict_salib_analyses[study_id][0][id_name][0,2],\n",
    "                           dict_salib_analyses[study_id][0][id_name][1,2]]\n",
    "            elif id == 2:\n",
    "                values = [dict_salib_analyses[study_id][0][id_name][0,1],\n",
    "                           dict_salib_analyses[study_id][0][id_name][0,2],\n",
    "                           dict_salib_analyses[study_id][0][id_name][1,2]]\n",
    "            rects = axs[i].bar(x + offset, values, width, label=legend_labels[study_id-2])\n",
    "            axs[i].set_xticks(x + width, s2_ticklabels, rotation=90)\n",
    "\n",
    "        axs[i].set_ylim(-0.5, 1.0)\n",
    "        axs[i].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        multiplier += 1\n",
    "        #axs[i].bar_label(rects, padding=3)\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "axs[0].set_ylabel('Index value')\n",
    "axs[0].set_title('Total Sobol indices')\n",
    "axs[1].set_title('First order Sobol indices')\n",
    "axs[2].set_title('Second order Sobol indices')\n",
    "axs[1].legend(loc='upper right')\n",
    "fig.suptitle('Comparison of Sobol indices for studies 2, 3, 4', fontsize=16)\n",
    "\n",
    "plt.show()\n",
    "plt.savefig('03_inspect/02_figs/sobol_indices_comparison_2_3_4.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_salib_analyses[7][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "# Compare 7 & 8\n",
    "#####\n",
    "\n",
    "plt.close('all')\n",
    "\n",
    "x = np.arange(10)  # the label locations\n",
    "width = 0.25  # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "s2_ticklabels = [i[0]+',\\n'+i[1] for i in itertools.combinations(dict_salib_problem[7][0]['names'], 2)]\n",
    "print(s2_ticklabels)\n",
    "legend_labels = ['Study 7', 'Study 8']\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3, layout='constrained', figsize=(12, 6))\n",
    "for i, id_name in enumerate(['ST', 'S1', 'S2']):\n",
    "#for i, id_name in enumerate(['S1']):\n",
    "    multiplier = 0\n",
    "    for study_id in range(7,9):\n",
    "        offset = width * multiplier\n",
    "        if id_name != 'S2':\n",
    "            #values = [dict_salib_analyses[2][0][id_name][id], dict_salib_analyses[3][0][id_name][id], dict_salib_analyses[4][0][id_name][id]]\n",
    "            #print(values)\n",
    "            values = dict_salib_analyses[study_id][0][id_name]\n",
    "            rects = axs[i].bar(x + offset, values, width, label=legend_labels[study_id-7])\n",
    "            axs[i].set_xticks(x + width, dict_salib_problem[7][0]['names'], rotation=90)\n",
    "        else:\n",
    "            values = [dict_salib_analyses[study_id][0][id_name][i,j] for i in range(0,10) for j in range(i+1, 10)] \n",
    "            print(values)\n",
    "            rects = axs[i].bar(np.arange(len(values)) + offset, values, width, label=legend_labels[study_id-7])\n",
    "            axs[i].set_xticks(np.arange(len(values)) + width, s2_ticklabels, rotation=90)\n",
    "\n",
    "        axs[i].set_ylim(-0.5, 1.0)\n",
    "        axs[i].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        multiplier += 1\n",
    "        #axs[i].bar_label(rects, padding=3)\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "axs[0].set_ylabel('Index value')\n",
    "axs[0].set_title('Total Sobol indices')\n",
    "axs[1].set_title('First order Sobol indices')\n",
    "axs[2].set_title('Second order Sobol indices')\n",
    "axs[1].legend(loc='upper right')\n",
    "fig.suptitle('Comparison of Sobol indices for studies 7, 8', fontsize=16)\n",
    "\n",
    "plt.show()\n",
    "plt.savefig('03_inspect/02_figs/sobol_indices_comparison_7_8.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict_salib_problem[7][0]\n",
    "dict_df_success_local_from_merged_studies[7][0].head()\n",
    "dict_df_merged_params_success_from_merged_studies[7][0].head()\n",
    "#dict_df_success_local_from_merged_studies[7][0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_df_merged_params_success_from_merged_studies[7][0].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_for_svd = ['annealing_time', 'programming_thermalization',\n",
    "       'readout_thermalization', 'flux_drift_compensation', 'chain_strength',\n",
    "       'anneal_offsets_1_qubits', 'anneal_offsets_2_qubits',\n",
    "       'anneal_offsets_3_qubits', 'a', 'b', 'fraction_samples_is_found_best']\n",
    "array_for_svd = dict_df_merged_params_success_from_merged_studies[7][0][cols_for_svd].to_numpy()\n",
    "print(array_for_svd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, Vh = scipy.linalg.svd(array_for_svd, lapack_driver='gesvd')\n",
    "sigma = np.zeros(array_for_svd.shape)\n",
    "for i in range(min(array_for_svd.shape)):\n",
    "    sigma[i, i] = S[i]\n",
    "assert np.allclose(array_for_svd, np.dot(U, np.dot(sigma, Vh)))\n",
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.printoptions(linewidth=400):\n",
    "    print(Vh[0, :], np.linalg.norm(Vh[0, :]))\n",
    "    for c, vh in zip(cols_for_svd, Vh[0, :]):\n",
    "        print(f'{c}:\\t {vh}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "# Compare cases in 1\n",
    "#####\n",
    "plt.close('all')\n",
    "\n",
    "x = np.arange(2)  # the label locations\n",
    "width = 0.25  # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "s2_ticklabels = [dict_salib_problem[1][0]['names'][0]+',\\n'+dict_salib_problem[1][0]['names'][1]]\n",
    "legend_labels = ['Study 1']\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3, layout='constrained', figsize=(12, 6))\n",
    "for i, id_name in enumerate(['ST', 'S1', 'S2']):\n",
    "#for i, id_name in enumerate(['S1']):\n",
    "    multiplier = 0\n",
    "    for study_id in range(1,2):\n",
    "        offset = width * multiplier\n",
    "        if id_name != 'S2':\n",
    "            #values = [dict_salib_analyses[2][0][id_name][id], dict_salib_analyses[3][0][id_name][id], dict_salib_analyses[4][0][id_name][id]]\n",
    "            #print(values)\n",
    "            values = dict_salib_analyses[study_id][0][id_name]\n",
    "            rects = axs[i].bar(x + offset, values, width, label=legend_labels[study_id-2])\n",
    "            axs[i].set_xticks(x + width, dict_salib_problem[1][0]['names'], rotation=90)\n",
    "        else:\n",
    "            values = [dict_salib_analyses[study_id][0][id_name][0,1]]\n",
    "            \n",
    "            rects = axs[i].bar((x[0] + offset,), values, width, label=legend_labels[study_id-2])\n",
    "            axs[i].set_xticks((x[0] + offset,), s2_ticklabels, rotation=90)\n",
    "\n",
    "        axs[i].set_ylim(-0.5, 1.0)\n",
    "        axs[i].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        multiplier += 1\n",
    "        #axs[i].bar_label(rects, padding=3)\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "axs[0].set_ylabel('Index value')\n",
    "axs[0].set_title('Total Sobol indices')\n",
    "axs[1].set_title('First order Sobol indices')\n",
    "axs[2].set_title('Second order Sobol indices')\n",
    "axs[1].legend(loc='upper right')\n",
    "fig.suptitle('Comparison of Sobol indices for variations in study 1', fontsize=16)\n",
    "\n",
    "plt.show()\n",
    "plt.savefig('03_inspect/02_figs/sobol_indices_comparison_1.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_df_merged_params_success_from_merged_studies[1][0].columns#[['num_samples_is_found_best', 'num_samples_matched_1_3', 'num_samples_matched_2_4']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "# Heatmap plot for cases in 1\n",
    "#####\n",
    "\n",
    "_df_sorted = dict_df_merged_params_success_from_merged_studies[1][0].sort_values(\n",
    "    by=['num_particles','num_nearest_neighbours'])\n",
    "_df_sorted.head()\n",
    "\n",
    "dict_df_merged_params_success_from_merged_studies[1][0].head()\n",
    "data_imshow = -1*np.ones((len(_df_sorted['num_particles'].unique()), len(_df_sorted['num_nearest_neighbours'].unique())))\n",
    "data_num_samples = np.zeros_like(data_imshow, dtype=int)\n",
    "for np_id in range(len(_df_sorted['num_particles'].unique())):\n",
    "    nump = _df_sorted['num_particles'].unique()[np_id]\n",
    "    for nn_id, numn in enumerate(_df_sorted[_df_sorted['num_particles'] == nump]['num_nearest_neighbours'].unique()):\n",
    "        _nn_id = np.where(_df_sorted['num_nearest_neighbours'].unique() == numn)[0][0]\n",
    "        data_imshow     [np_id, _nn_id] = _df_sorted[(_df_sorted['num_particles'] == nump) & (_df_sorted['num_nearest_neighbours'] == numn)]['fraction_samples_is_found_best'].values[0]\n",
    "        data_num_samples[np_id, _nn_id] = _df_sorted[(_df_sorted['num_particles'] == nump) & (_df_sorted['num_nearest_neighbours'] == numn)]['num_samples_is_found_best'].values[0]\n",
    "#data_imshow *= 100\n",
    "print(data_imshow.shape, data_imshow)\n",
    "print(data_num_samples.shape, data_num_samples)\n",
    "fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(10, 6))\n",
    "cmap = matplotlib.colormaps.get_cmap(\"viridis\")\n",
    "cmap.set_bad(color='white')\n",
    "\n",
    "im = axs.imshow(np.ma.masked_where(data_imshow == -1, data_imshow), cmap=cmap, vmin=0.0)\n",
    "axs.set_title('Fraction of samples found best for different number of particles and nearest neighbours')\n",
    "axs.set_xlabel('Number of nearest neighbours')\n",
    "axs.set_ylabel('Number of particles')\n",
    "axs.set_xticks(range(data_imshow.shape[1]), labels=_df_sorted['num_nearest_neighbours'].unique(),\n",
    "                rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "axs.set_yticks(range(data_imshow.shape[0]), labels=_df_sorted['num_particles'].unique())\n",
    "for i in range(data_imshow.shape[0]):\n",
    "        for j in range(data_imshow.shape[1]):\n",
    "            if _df_sorted['num_nearest_neighbours'].unique()[j] in _df_sorted[(_df_sorted['num_particles'] == _df_sorted['num_particles'].unique()[i])]['num_nearest_neighbours'].values:\n",
    "                color = 'w' if i!=0 or j!=0 else 'k'\n",
    "                text = axs.text(j, i, format(data_imshow[i, j], \".2e\") + '\\n' + str(data_num_samples[i, j]),\n",
    "                        ha=\"center\", va=\"center\", color=color, fontsize=6)\n",
    "cbar = axs.figure.colorbar(im, ax=axs, fraction=0.02, pad=0.04)\n",
    "fig.tight_layout()\n",
    "fig.savefig('03_inspect/02_figs/heatmap_study_1.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "# Simple plots for cases in 1\n",
    "#####\n",
    "\n",
    "type(np.ma.masked_where(data_imshow == -1, data_imshow)[0,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.printoptions(precision=3, linewidth=400):\n",
    "    print(np.ma.masked_where(data_imshow == -1, data_imshow)[0,5])\n",
    "    for k, v in dict_salib_problem.items():\n",
    "        print('study:', k)\n",
    "        for kk, vv in v.items():\n",
    "            print(' substudy:', kk)\n",
    "            for kkk, vvv in vv.items():\n",
    "                print('  ', kkk, vvv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dict_df_merged_params_success_from_merged_studies[2][0]['fraction_samples_is_found_best'].max())\n",
    "#print(dict_df_merged_params_success_from_merged_studies[2][0]['fraction_samples_matched_1_1'].max())\n",
    "print(dict_df_merged_params_success_from_merged_studies[2][0]['num_samples_matched_corrected_1_2'].max()/dict_df_merged_params_success_from_merged_studies[2][0]['num_samples'].sum())\n",
    "print(dict_df_merged_params_success_from_merged_studies[2][0]['num_samples'].sum())\n",
    "#print(dict_df_merged_params_success_from_merged_studies[2][0]['fraction_samples_matched_2_1'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_cache_dict_sim_anneling_solutions[2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fallback_found_0_update_ij(dict_df,i,j,_key0):\n",
    "    _newj = str(int(j)-1)\n",
    "    _key = _key0 + '_' + i + '_' + _newj\n",
    "    _returnval = dict_df[study_id][substudy_id][_key].max()\n",
    "    if _returnval == 0.0:\n",
    "        _returnval = _fallback_found_0_update_ij(dict_df,i,_newj,_key0)\n",
    "    return _returnval\n",
    "\n",
    "def get_fractions_for_barplot(dict_df, study_id, substudy_id, key):\n",
    "    keysplit = key.split('_')\n",
    "    i, j = keysplit[-2], keysplit[-1]\n",
    "    #print(i,j)\n",
    "    returnval = -99.\n",
    "    if False:# i == '1' and j == '1':\n",
    "        returnval = dict_df[study_id][substudy_id]['fraction_samples_is_found_best'].max()\n",
    "    else:\n",
    "        _key = '_'.join(keysplit[:-2]) + '_' + i + '_' + j\n",
    "        returnval = dict_df[study_id][substudy_id][_key].max()\n",
    "        while returnval == 0.0:\n",
    "            j = str(int(j)-1)\n",
    "            if i=='0' and j=='0':\n",
    "                returnval = dict_df[study_id][substudy_id]['fraction_samples_is_found_best'].max()\n",
    "            else:\n",
    "                returnval = dict_df[study_id][substudy_id]['_'.join(keysplit[:-2]) + '_' + i + '_' + j].max()\n",
    "        #if returnval == 0.0:\n",
    "        #    returnval = _fallback_found_0_update_ij(dict_df,i,j,'_'.join(keysplit[:-2]))\n",
    "    if study_id == 5 and substudy_id == 0:\n",
    "        print(_key, returnval)\n",
    "        print('_'.join(keysplit[:-2]) + '_' + i + '_' + j)\n",
    "        print(dict_df[study_id][substudy_id]['_'.join(keysplit[:-2]) + '_' + i + '_' + j].max())\n",
    "\n",
    "    return returnval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "studies = list(dict_df_merged_params_success_from_merged_studies.keys())\n",
    "substudies = [(i,j) for i in studies for j in dict_df_merged_params_success_from_merged_studies[i].keys() if i > 1]\n",
    "species = tuple((s[0],s[1]+1) for s in substudies) # groups\n",
    "penguin_means = {\n",
    "#    f'fraction_samples_matched_{i}_{j}': None for i in n_samples_to_compare for j in n_exact_sols_to_compare if i<4 and j<4}\n",
    "    f'fraction_samples_matched_corrected_{i+1}_{j+1}': None for i in range(n_samples_to_compare[0]) for j in range(n_exact_sols_to_compare[0]) if j>=i and j<5}\n",
    "for k in list(penguin_means.keys()):\n",
    "    #penguin_means[k] = [100*dict_df_merged_params_success_from_merged_studies[i][j][k].max() for (i,j) in substudies]\n",
    "    penguin_means[k] = [100*get_fractions_for_barplot(dict_df_merged_params_success_from_merged_studies, i, j, k) for (i,j) in substudies]\n",
    "penguin_means['num_data'] = [dict_df_merged_params_success_from_merged_studies[i][j]['identifiers'].size/1000 for (i,j) in substudies]\n",
    "\n",
    "x = 5.5*np.arange(len(species))  # the label locations\n",
    "width = 0.45  # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(17, 6))\n",
    "#fig, ax = plt.subplots()\n",
    "\n",
    "for attribute, measurement in penguin_means.items():\n",
    "    offset = width * multiplier\n",
    "    label = attribute.split('_')\n",
    "    if label[0] == 'fraction':\n",
    "        label = f\"QA: {label[-2]}, classical: {label[-1]}\"\n",
    "    elif label[0] == 'num':\n",
    "        label = f\"number parameter sets\"\n",
    "    rects = ax.bar(x + offset, measurement, width, label=label)\n",
    "    ax.bar_label(rects, padding=3, fontsize=7, rotation=90)\n",
    "    multiplier += 1\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Fraction of samples [%],\\nNumber of successfully sampled parameter sets [x1000]')\n",
    "ax.set_xlabel('(Study, substudy)')\n",
    "ax.set_title('Fractions of samples of all annealer parameter studies and substudies')\n",
    "ax.set_xticks(x + width + (len(species)-3)*width/2, species)\n",
    "ax.legend()\n",
    "#ax.legend(loc='upper left', ncols=3)\n",
    "ax.set_ylim(0, 12.75)\n",
    "#fig.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig('03_inspect/02_figs/fraction_samples_all_studies.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in dict_df_merged_params_success_from_merged_studies.items():\n",
    "    print('study:', k)\n",
    "    for kk, vv in v.items():\n",
    "        print(' substudy:', kk)\n",
    "        print('  fraction_samples_is_found_best:')\n",
    "        print(vv['fraction_samples_is_found_best'].sort_values(ascending=False).head())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in dict_df_merged_params_success_from_merged_studies[7][0].columns:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_to_exclude = ['identifiers', 'num_data', *[f't{i:02d}' for i in range(12)], *[f's{i:02d}' for i in range(12)], 'set_id', 'num_samples_is_found_best_per_run',\n",
    "                     'is_found_best_per_run', 'num_subs_per_run', 'num_samples_per_run', 'num_samples_per_sub_per_run', 'submissions',\n",
    "                     'fraction_samples_is_found_best_per_run',\n",
    "                     *[f'num_samples_matched_per_run_{i}_{j}' for i in range(1, 6) for j in range(1, 6)],\n",
    "                     *[f'num_matched_per_run_{i}_{j}' for i in range(1, 6) for j in range(1, 6)],\n",
    "                     *[f'fraction_samples_matched_per_run_{i}_{j}' for i in range(1, 6) for j in range(1, 6)],\n",
    "                     *[f'num_samples_matched_per_sub_per_run_{i}_{j}' for i in range(1, 6) for j in range(1, 6)],\n",
    "                     *[f'num_matched_per_sub_per_run_{i}_{j}' for i in range(1, 6) for j in range(1, 6)]]\n",
    "#names_to_exclude\n",
    "cols_to_keep = [col for col in dict_df_merged_params_success_from_merged_studies[7][0].columns if col not in names_to_exclude]\n",
    "dict_df_for_corr = dict_df_merged_params_success_from_merged_studies[7][0][cols_to_keep]\n",
    "dict_df_for_corr.corr(method='pearson')\n",
    "# for name in dict_df_for_corr.columns:\n",
    "#     print(name, dict_df_for_corr[name].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_df_for_corr_normalized =(dict_df_for_corr-dict_df_for_corr.mean())/dict_df_for_corr.std()\n",
    "dict_df_for_corr_normalized.corr(method='pearson')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
