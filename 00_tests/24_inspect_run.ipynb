{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pathlib\n",
    "sys.path.append(str(pathlib.PurePath(pathlib.Path.cwd().parent)))\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.axes as am\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget\n",
    "\n",
    "import dimod\n",
    "import dwave\n",
    "import dwave.system\n",
    "from dwave.system import DWaveSampler, EmbeddingComposite, FixedEmbeddingComposite\n",
    "import dwave.inspector\n",
    "import dwave_networkx as dnx\n",
    "import minorminer\n",
    "\n",
    "from src.particle_funcs import distance_matrix as distance_matrix\n",
    "from src.particle_funcs import io as particles_io\n",
    "import src.leap_funcs.qubo.q_matrix as q_matrix\n",
    "\n",
    "from src import leap_funcs as leap_funcs\n",
    "from src.leap_funcs import embedding_quality\n",
    "from src.leap_funcs.qubo import parameterstudy\n",
    "\n",
    "from src import h5py_funcs\n",
    "from src.h5py_funcs import inspections, discoveries, init_custom_getstates, io, parameterstudy_using_info_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_name = 'sub_2'\n",
    "info_file_name = r'study_params_small.h5'\n",
    "study_name = 'sub_3'\n",
    "study_name = 'sub_4'\n",
    "study_name = 'sub_5'\n",
    "study_name = 'sub_5_1_1'\n",
    "study_name = 'sub_5_2'\n",
    "study_name = 'sub_5_3'\n",
    "#study_name = 'sub_5_3_2'\n",
    "\n",
    "study_subs = ['', '_2']\n",
    "study_subs = ['']\n",
    "\n",
    "info_name = r'study_params_sub5_3'\n",
    "\n",
    "info_file_name = [info_name + sub + '.h5' for sub in study_subs]\n",
    "\n",
    "\n",
    "study_folder_path = [pathlib.Path.cwd().joinpath('01_out\\\\'+study_name+sub) for sub in study_subs]\n",
    "\n",
    "info_file_name_path = [pathlib.Path.joinpath(sfp, ifn) for sfp, ifn in zip(study_folder_path, info_file_name)]\n",
    "samples_folder_name = r'samples'\n",
    "samples_folder_name_path = [pathlib.Path.joinpath(sfp, samples_folder_name) for sfp in study_folder_path]\n",
    "print(str(info_file_name_path[0])[-3:])\n",
    "assert np.array([i.exists() for i in info_file_name_path]).all(), f'Info file does not exist. Check info_file_name_path ({info_file_name_path})'\n",
    "info_file_name_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_infos_read = {i:h5py_funcs.inspections.read_info_file_to_dict(info_file_name_path=ifp, infoset_name = 'info')  for i,ifp in enumerate(info_file_name_path)}\n",
    "#dddddd = dict_infos_read.copy()\n",
    "\n",
    "dict_info_read = {}\n",
    "#for key in reversed(dict_infos_read.keys()): #reversed to make sure info from initial study_file is contained as a|=b priorizes b\n",
    "#    dict_info_read = dict_info_read | dddddd[key]\n",
    "\n",
    "for i,ifp in enumerate(info_file_name_path):\n",
    "    dict_info_read.update(h5py_funcs.inspections.read_info_file_to_dict(info_file_name_path=ifp, infoset_name = 'info'))\n",
    "\n",
    "#dict_info_read\n",
    "\n",
    "\n",
    "#orig:\n",
    "#dict_info_read = h5py_funcs.inspections.read_info_file_to_dict(info_file_name_path=info_file_name_path, infoset_name = 'info')\n",
    "#dict_info_read.keys()\n",
    "\n",
    "##########\n",
    "#\n",
    "# The following test fails, because comparing dicts throws an error (below) when arrays are encountered, thats why it is left to the user to make sure the subs are compatible with the main study.\n",
    "# \"The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\" \n",
    "#\n",
    "#########\n",
    "#if len(info_file_name_path)>1:\n",
    "#    for i in range(len(info_file_name_path)):\n",
    "#        for key, value in dict_infos_read[0].items():\n",
    "#            if key=='time_history': continue\n",
    "#            else:\n",
    "#                print(key)\n",
    "#                if isinstance(value, np.ndarray):\n",
    "#                    assert (value == dict_infos_read[i][key]).all(), print(value, dict_infos_read[i][key])\n",
    "#                else:\n",
    "#                    try: value == dict_infos_read[i][key]\n",
    "#                    except Exception as e:\n",
    "#                        print(e)\n",
    "#                        print(value)\n",
    "#                        print(dict_infos_read[i][key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(dict_infos_read[0]['time_history'])\n",
    "for i in range(len(dict_infos_read)):\n",
    "    _tmp = []\n",
    "    for key, value in dict_infos_read[i]['time_history'].items():\n",
    "        if key == 'attrs':\n",
    "            continue\n",
    "        else:\n",
    "            if not value['attrs']['finished']:\n",
    "                _tmp.append(key)\n",
    "    print(len(_tmp), _tmp)\n",
    "del _tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_studies = list(dict_info_read['time_history'].keys())\n",
    "list_studies.remove('attrs')\n",
    "print(len(list_studies), list_studies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(dict_info_read['time_history']['zz_7293366434']['data'][2].decode('utf-8')[:4] == '2024')\n",
    "#print(dict_info_read['time_history']['zz_7293366434']['attrs']['finished'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stud in list_studies:\n",
    "    if (not dict_info_read['time_history'][stud]['attrs']['finished'])\\\n",
    "        and (dict_info_read['time_history'][stud]['data'][2].decode('utf-8')[:4] == '2024'):\n",
    "        print('here 1', stud)\n",
    "    elif dict_info_read['time_history'][stud]['attrs']['finished']\\\n",
    "        and (dict_info_read['time_history'][stud]['data'][2].decode('utf-8')[:4] != '2024'):\n",
    "        print('here 1_2', stud)\n",
    "    elif dict_info_read['time_history'][stud]['attrs']['finished']\\\n",
    "        and (dict_info_read['time_history'][stud]['data'][2].decode('utf-8')[:4] == '2024'):\n",
    "        #print('here 2')\n",
    "        continue\n",
    "    else:\n",
    "        print('here else', stud)\n",
    "        for key, value in dict_infos_read.items():\n",
    "            if '2024' == value['time_history'][stud]['data'][2].decode('utf-8')[:4]:\n",
    "                dict_info_read['time_history'][stud]['data'][1] = (value['time_history'][stud]['data'][1]+'.'.encode('utf-8'))[:-1]\n",
    "                dict_info_read['time_history'][stud]['data'][2] = (value['time_history'][stud]['data'][2]+'.'.encode('utf-8'))[:-1]\n",
    "                dict_info_read['time_history'][stud]['attrs']['finished'] = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stud in list_studies:\n",
    "    if not dict_info_read['time_history'][stud]['attrs']['finished']:\n",
    "        print(stud, dict_info_read['time_history'][stud]['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dict_info_read['study'])\n",
    "study_read = dict_info_read['study']['data']\n",
    "study_read.dtype.names\n",
    "est_runts = study_read['sets']['estimated_runtime']\n",
    "cum_est_runts = np.sum(est_runts)\n",
    "print(est_runts)\n",
    "print('cummulative estimated runtime for 1000 samples each is [h]:', cum_est_runts/3600)\n",
    "del study_read, est_runts, cum_est_runts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_identifiers, started_psets, finished_psets = h5py_funcs.inspections.extract_identifiers(dict_info_read = dict_info_read)\n",
    "\n",
    "print(started_psets)\n",
    "print(finished_psets)\n",
    "print('{p1}/{p2} p_sets have been started, \\n {p3}/{p2} p_sets have been finished'.format(p1=len(started_psets), p2=array_identifiers.shape[0], p3=len(finished_psets)))\n",
    "\n",
    "list_array_identifiers = []; list_started_psets = []; list_finished_psets = []\n",
    "appenders = [list_array_identifiers, list_started_psets, list_finished_psets]\n",
    "for d in dict_infos_read.values():\n",
    "    appendees = h5py_funcs.inspections.extract_identifiers(dict_info_read = d)\n",
    "    print(len(appendees[0]), len(appendees[1]), len(appendees[2]))\n",
    "    assert (array_identifiers==appendees[0]).all()\n",
    "    list_array_identifiers.append(appendees[0])\n",
    "    if len(list_started_psets) > 0:\n",
    "        print('here if')\n",
    "        list_started_psets.append([ap for ap in appendees[1] if not ap in list_started_psets[0]])\n",
    "        list_finished_psets.append([ap for ap in appendees[2] if not ap in list_finished_psets[0]])\n",
    "    else:\n",
    "        print('here else')\n",
    "        list_started_psets.append(appendees[1])\n",
    "        list_finished_psets.append(appendees[2])\n",
    "\n",
    "print(list_started_psets)\n",
    "print(list_finished_psets)\n",
    "print('{p1}/{p2} p_sets have been started, \\n {p3}/{p2} p_sets have been finished'.\\\n",
    "      format(p1=[len(l) for l in list_started_psets], p2=[len(l) for l in list_array_identifiers], p3=[len(l) for l in list_finished_psets]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_read_from_sample_data = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_for_df = {}\n",
    "if is_read_from_sample_data:\n",
    "    import os\n",
    "    for sfnp, ids in zip(samples_folder_name_path,list_finished_psets):\n",
    "        print('reading from', sfnp)\n",
    "        #print(os.path.exists(sfnp))\n",
    "        #for id in ids:\n",
    "        #    _tmp = os.path.join(sfnp, id.decode('utf-8')+'.h5')\n",
    "        #    print(os.path.exists(_tmp), _tmp)\n",
    "        dict_for_df |= h5py_funcs.inspections.read_answers_to_dict(\\\n",
    "            samples_folder_name_path=sfnp\\\n",
    "            , array_identifiers=np.array(ids))#[::25]\n",
    "    print(dict_for_df.__sizeof__()) # size in memory in bytes\n",
    "    print(len(dict_for_df)) # number of keys in dict\n",
    "    with open(f'01_out\\\\dict_for_df_{study_name}.txt', 'wb') as f:\n",
    "        pickle.dump(dict_for_df, f, protocol=5)\n",
    "    del dict_for_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_read_from_sample_data:\n",
    "    with open(f'01_out\\\\dict_for_df_{study_name}.txt', 'rb') as f:\n",
    "        dict_for_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_combine_pickled_dicts = False\n",
    "if is_combine_pickled_dicts:\n",
    "    in_dicts = ['01_out\\\\dict_for_df_sub_5_3.txt'\\\n",
    "               ,'01_out\\\\dict_for_df_sub_5_3_2.txt']\n",
    "    out_dicts = ['01_out\\\\dict_for_df_sub_5_3_combined.txt']\n",
    "    out_dict = {}\n",
    "    for in_d in in_dicts:\n",
    "        print(f'read {in_d}')\n",
    "        with open(in_d, 'rb') as f:\n",
    "            out_dict |= pickle.load(f)\n",
    "    print('finished reading all in_dicts')\n",
    "    for key, val in out_dict.items():\n",
    "        sample_set_length = (len(val['custom']['sampleset'].keys()))\n",
    "        if sample_set_length != 10:\n",
    "            print(key, sample_set_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_combine_pickled_dicts:\n",
    "    out_dicts = ['01_out\\\\dict_for_df_sub_5_3_combined.txt']\n",
    "    with open(out_dicts[0], 'wb') as f:\n",
    "        pickle.dump(out_dict, f, protocol=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import dwave.samplers\n",
    "\n",
    "num_particles = 5\n",
    "qubo = dict_info_read['qubos'][f'{num_particles}_{num_particles}']\n",
    "\n",
    "#sim_annealing_sample = dimod.samplers.ExactSolver().sample_qubo(\n",
    "#    {ast.literal_eval(key): value['data'] for key, value in qubo.items()})\n",
    "sim_annealing_sample = dwave.samplers.SimulatedAnnealingSampler().sample_qubo(\n",
    "    {ast.literal_eval(key): value['data'] for key, value in qubo.items()},\n",
    "    num_reads=10000)\n",
    "sim_annealing_sample = sim_annealing_sample.aggregate() # accumulates number of occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sim_annealing_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_sol = sim_annealing_sample.record\n",
    "exact_sol.sort(order='energy')\n",
    "exact_sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_to_compare = 3\n",
    "n_exact_sols_to_compare = 3\n",
    "#colour_label = 'fraction_samples_matched_3_samps_3_sols'\n",
    "colour_label = 'fraction_samples_is_found_best'\n",
    "dir_name_path_plots = './01_out/'\n",
    "is_train_gp = False\n",
    "training_iterations = 5 # only required if is_train_gp==True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success_dict = h5py_funcs.inspections.extract_success_dict(\\\n",
    "    dict_for_df = dict_for_df\\\n",
    "    , exact_sols = exact_sol\\\n",
    "    , n_samples_to_compare = n_samples_to_compare\\\n",
    "    , n_exact_sols_to_compare = n_exact_sols_to_compare)\n",
    "\n",
    "\n",
    "for key in success_dict.keys():\n",
    "    print(key, success_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig = h5py_funcs.inspections.return_bar_plot(\\\n",
    "#    success_dict = success_dict\\\n",
    "#    , n_samples_to_compare = n_samples_to_compare\\\n",
    "#    , n_exact_sols_to_compare = n_exact_sols_to_compare)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros((dict_info_read['study']['data'].shape[0],))\n",
    "print(a)\n",
    "print(dict_info_read['study']['data']['sets'].shape)\n",
    "print(dict_info_read['study']['data']['sets'].dtype.isalignedstruct)\n",
    "b = np.lib.recfunctions.rec_append_fields(dict_info_read['study']['data'], 'param 1', a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(b.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_info_read['study']['data']['sets'].copy().view(dtype=np.float64, type=np.ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "started_sets, study_matched_started_ids = h5py_funcs.inspections.extract_started_sets_from_success_dict(\\\n",
    "    success_dict = success_dict\\\n",
    "    , dict_info_read = dict_info_read)\n",
    "print(started_sets)\n",
    "print(study_matched_started_ids)\n",
    "print(study_matched_started_ids.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_names = list(list(success_dict.values())[0].keys())\n",
    "to_remove = ['is_found_best_per_run', 'num_subs_per_run', 'num_samples_per_run', 'num_samples_per_sub_per_run', 'num_matched_per_run'\\\n",
    "             , 'num_matched_per_sub_per_run', 'num_samples_matched_per_run', 'num_samples_matched_per_sub_per_run', 'submissions', 'num_samples_is_found_best_per_run']\n",
    "for s in to_remove:\n",
    "    results_names.remove(s)\n",
    "print(results_names)\n",
    "\n",
    "sampler_array,results_names, ids = h5py_funcs.inspections.generate_sampler_array_for_plots(\\\n",
    "    success_dict = success_dict\\\n",
    "    , results_names = results_names\\\n",
    "    , started_sets = started_sets\\\n",
    "    , study_matched_started_ids = study_matched_started_ids\\\n",
    "    , n_samples_to_compare = n_samples_to_compare\\\n",
    "    , n_exact_sols_to_compare = n_exact_sols_to_compare)\n",
    "\n",
    "print(sampler_array.shape)\n",
    "print(results_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gpytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultitaskGPModel(gpytorch.models.ExactGP):\n",
    "                def __init__(self, train_x, train_y, likelihood):\n",
    "                    super(MultitaskGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "                    self.mean_module = gpytorch.means.MultitaskMean(\n",
    "                        gpytorch.means.ConstantMean(), num_tasks=sampler_array.shape[1]\n",
    "                    )\n",
    "                    self.covar_module = gpytorch.kernels.MultitaskKernel(\n",
    "                        gpytorch.kernels.RBFKernel(), num_tasks=sampler_array.shape[1]\n",
    "                    )\n",
    "\n",
    "                def forward(self, x):\n",
    "                    mean_x = self.mean_module(x)\n",
    "                    covar_x = self.covar_module(x)\n",
    "                    return gpytorch.distributions.MultitaskMultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "            # Instantiate multi-task likelihood and GP model \n",
    "if study_name == 'sub_2':\n",
    "    train_x = torch.tensor(study_matched_started_ids['sets'].view(np.float64).copy()).to(torch.float)\n",
    "    test_x = torch.tensor(dict_info_read['study']['data']['sets'].view(np.float64).copy()).to(torch.float)\n",
    "else:\n",
    "    _tmp_train_dtype = study_matched_started_ids['sets'].dtype\n",
    "    _tmp_test_dtype = dict_info_read['study']['data']['sets'].dtype\n",
    "    _tmp_train_data = study_matched_started_ids['sets'].view((np.float64, len(_tmp_train_dtype.names))).copy()\n",
    "    _tmp_test_data = dict_info_read['study']['data']['sets'].view((np.float64, len(_tmp_test_dtype.names))).copy()\n",
    "    train_x = torch.tensor(_tmp_train_data).to(torch.float)\n",
    "    test_x = torch.tensor(_tmp_test_data).to(torch.float)\n",
    "    del(_tmp_train_dtype, _tmp_test_dtype, _tmp_train_data, _tmp_test_data)\n",
    "\n",
    "train_y = torch.tensor(sampler_array).to(torch.float)\n",
    "\n",
    "num_params_in_gp = train_x.shape[1]\n",
    "num_params_out_gp = train_y.shape[1]\n",
    "\n",
    "likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=sampler_array.shape[1])\n",
    "model = MultitaskGPModel(train_x, train_y, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_train_gp:\n",
    "    print('is_train_gp =', is_train_gp, ' GP training is skipped')\n",
    "    mean = None\n",
    "    pass\n",
    "else:\n",
    "    # Switch to training mode\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    # Define optimizer and loss function\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    # Train GP using training data\n",
    "    print('Start training')\n",
    "    print(training_iterations)\n",
    "    for i in range(training_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('  Finished training iteration %i/%i' % (i + 1, training_iterations), 'loss:', loss.item())\n",
    "    print('Finished training' + '\\n')\n",
    "    # Switch to evaluation mode, and probe trained GP using testing data\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    print('Start testing')\n",
    "    with torch.no_grad():\n",
    "        observed_model = model(test_x)\n",
    "        print('  Testing: Finished evaluation')\n",
    "        observed_pred = likelihood(observed_model)\n",
    "        print('  Testing: Finished likelihood')\n",
    "        mean = observed_pred.mean\n",
    "        lower, upper = observed_pred.confidence_region()\n",
    "    print('Finished testing' + '\\n')\n",
    "\n",
    "    assert mean.shape[1] == num_params_out_gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(h5py_funcs.inspections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_SIZE = 10\n",
    "MEDIUM_SIZE = 12\n",
    "BIGGER_SIZE = 14\n",
    "kwargs_pltrc = {'font_size':SMALL_SIZE\\\n",
    "                ,'axes_titlesize': SMALL_SIZE\\\n",
    "                ,'axes_labelsize': MEDIUM_SIZE\\\n",
    "                ,'xtick_labelsize': SMALL_SIZE\\\n",
    "                ,'ytick_labelsize': SMALL_SIZE\\\n",
    "                ,'legend_fontsize': SMALL_SIZE\\\n",
    "                ,'figure_titlesize': BIGGER_SIZE\\\n",
    "                ,'marker_scatter': '2'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_names)\n",
    "print(mean.shape) if np.array(mean).all() else print('GP was not trained')\n",
    "print(colour_label)\n",
    "print(dir_name_path_plots)\n",
    "print(sampler_array.shape)\n",
    "print(study_matched_started_ids.dtype)\n",
    "print(study_matched_started_ids.ndim)\n",
    "print(len(study_matched_started_ids['sets'].dtype.names))\n",
    "print(study_matched_started_ids['sets'].ndim)\n",
    "fig2, fig_pp1 = h5py_funcs.inspections.return_plots(study_name = study_name\\\n",
    "        , study_matched_started_ids = study_matched_started_ids\\\n",
    "        , gp_mean = mean, results_names = results_names, dict_info_read = dict_info_read\\\n",
    "        , sampler_array = sampler_array, colour_label = colour_label, dir_name_path_plots = dir_name_path_plots\\\n",
    "        , comb_types=['bi'], kwargs_pltrc=kwargs_pltrc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig2 = fig2.sort_values(by='fraction_samples_is_found_best')\n",
    "#fig2\n",
    "#fig_pp1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig2.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig_pp1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SALib as sa\n",
    "import SALib.analyze.sobol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = study_matched_started_ids['sets'].dtype.names[:3]\n",
    "bounds = [[study_matched_started_ids['sets'][name].min(), study_matched_started_ids['sets'][name].max()] for name in names]\n",
    "salib_problem = {\n",
    "    'num_vars': 3,\n",
    "    'names': names,\n",
    "    'bounds': bounds\n",
    "}\n",
    "salib_problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar = sampler_array[:,-2]\n",
    "sa.analyze.sobol.analyze(salib_problem, np.pad(ar, (0, 512-len(ar)), 'constant', constant_values=(0, ar.mean())), print_to_console=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar = sampler_array[:,-2]\n",
    "ar2 = np.pad(ar, (0, 512-len(ar)), 'constant', constant_values=(0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler_array, names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
