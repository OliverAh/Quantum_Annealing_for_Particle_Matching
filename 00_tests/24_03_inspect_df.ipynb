{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_section(text):\n",
    "    print('##############################################')\n",
    "    print('###  ', text)\n",
    "    print('##############################################')\n",
    "\n",
    "\n",
    "print_section('Start imports')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "sys.path.append(str(pathlib.PurePath(pathlib.Path.cwd().parent)))\n",
    "\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.axes as am\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget\n",
    "\n",
    "\n",
    "from src.particle_funcs import distance_matrix as distance_matrix\n",
    "from src.particle_funcs import io as particles_io\n",
    "import src.leap_funcs.qubo.q_matrix as q_matrix\n",
    "\n",
    "from src import leap_funcs as leap_funcs\n",
    "from src.leap_funcs import embedding_quality\n",
    "from src.leap_funcs.qubo import parameterstudy\n",
    "\n",
    "from src import h5py_funcs\n",
    "from src.h5py_funcs import inspections, discoveries, init_custom_getstates, io, parameterstudy_using_info_file\n",
    "\n",
    "print_section('Finished imports')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "print('io.DEFAULT_BUFFER_SIZE', io.DEFAULT_BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section('Gather file and folder paths - Start')\n",
    "info_file_name = r'study_params_small.h5'\n",
    "#study_name = 'sub_2'\n",
    "#study_name = 'sub_3'\n",
    "#study_name = 'sub_4_2'\n",
    "#study_name = 'sub_5'\n",
    "#study_name = 'sub_5_1_1'\n",
    "#study_name = 'sub_5_2'\n",
    "#study_name = 'sub_5_3'\n",
    "#study_name = 'sub_5_3_2'\n",
    "#study_name = 'sub_7_1_2'\n",
    "study_name = 'sub_7_1'\n",
    "\n",
    "study_subs = ['', '_2', '_3', '_4', '_5']\n",
    "#study_subs = ['']\n",
    "\n",
    "if study_name == 'sub_1':\n",
    "    ...\n",
    "elif study_name == 'sub_2':\n",
    "    info_name = r'study_params_small'\n",
    "else:\n",
    "    info_name = r'study_params_' + study_name[:3] + study_name[4:]\n",
    "#info_name = r'study_params_sub7_1_5'\n",
    "\n",
    "info_file_name = [info_name + sub + '.h5' for sub in study_subs]\n",
    "\n",
    "\n",
    "study_folder_path = [pathlib.Path.cwd().joinpath('01_out',study_name+sub) for sub in study_subs]\n",
    "print('study_folder_path:', study_folder_path)\n",
    "\n",
    "info_file_name_path = [pathlib.Path.joinpath(sfp, ifn) for sfp, ifn in zip(study_folder_path, info_file_name)]\n",
    "print('info_file_name_path:', info_file_name_path)\n",
    "samples_folder_name = r'samples'\n",
    "samples_folder_name_path = [pathlib.Path.joinpath(sfp, samples_folder_name) for sfp in study_folder_path]\n",
    "print('samples_folder_name_path:', samples_folder_name_path)\n",
    "print(str(info_file_name_path[0])[-3:])\n",
    "assert np.array([i.exists() for i in info_file_name_path]).all(), f'Info file does not exist. Check info_file_name_path ({info_file_name_path})'\n",
    "print(info_file_name_path)\n",
    "print_section('Gather file and folder paths - Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section('Read info files - Start')\n",
    "dict_infos_read = {i:h5py_funcs.inspections.read_info_file_to_dict(info_file_name_path=ifp, infoset_name = 'info')  for i,ifp in enumerate(info_file_name_path)}\n",
    "#dddddd = dict_infos_read.copy()\n",
    "\n",
    "dict_info_read = {}\n",
    "#for key in reversed(dict_infos_read.keys()): #reversed to make sure info from initial study_file is contained as a|=b priorizes b\n",
    "#    dict_info_read = dict_info_read | dddddd[key]\n",
    "\n",
    "for i,ifp in enumerate(info_file_name_path):\n",
    "    dict_info_read.update(h5py_funcs.inspections.read_info_file_to_dict(info_file_name_path=ifp, infoset_name = 'info'))\n",
    "\n",
    "print_section('Read info files - Finished')\n",
    "\n",
    "#dict_info_read\n",
    "\n",
    "\n",
    "#orig:\n",
    "#dict_info_read = h5py_funcs.inspections.read_info_file_to_dict(info_file_name_path=info_file_name_path, infoset_name = 'info')\n",
    "#dict_info_read.keys()\n",
    "\n",
    "##########\n",
    "#\n",
    "# The following test fails, because comparing dicts throws an error (below) when arrays are encountered, thats why it is left to the user to make sure the subs are compatible with the main study.\n",
    "# \"The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\" \n",
    "#\n",
    "#########\n",
    "#if len(info_file_name_path)>1:\n",
    "#    for i in range(len(info_file_name_path)):\n",
    "#        for key, value in dict_infos_read[0].items():\n",
    "#            if key=='time_history': continue\n",
    "#            else:\n",
    "#                print(key)\n",
    "#                if isinstance(value, np.ndarray):\n",
    "#                    assert (value == dict_infos_read[i][key]).all(), print(value, dict_infos_read[i][key])\n",
    "#                else:\n",
    "#                    try: value == dict_infos_read[i][key]\n",
    "#                    except Exception as e:\n",
    "#                        print(e)\n",
    "#                        print(value)\n",
    "#                        print(dict_infos_read[i][key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section('Check read info files for unfinished runs - Start')\n",
    "#print(dict_infos_read[0]['time_history'])\n",
    "for i in range(len(dict_infos_read)):\n",
    "    _tmp = []\n",
    "    for key, value in dict_infos_read[i]['time_history'].items():\n",
    "        if key == 'attrs':\n",
    "            continue\n",
    "        else:\n",
    "            if not value['attrs']['finished']:\n",
    "                _tmp.append(key)\n",
    "    print('unfinished param sets:', len(_tmp), _tmp)\n",
    "print('done with separate dicts, now go with merged dict')\n",
    "_tmp = []\n",
    "for key, value in dict_info_read['time_history'].items():\n",
    "    if key == 'attrs':\n",
    "        continue\n",
    "    else:\n",
    "        if not value['attrs']['finished']:\n",
    "            _tmp.append(key)\n",
    "print('unfinished param sets:', len(_tmp), _tmp)\n",
    "del _tmp\n",
    "print_section('Check read info files for unfinished runs - Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_studies = list(dict_info_read['time_history'].keys())\n",
    "list_studies.remove('attrs')\n",
    "print('list of all studies:', len(list_studies), list_studies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(dict_info_read['time_history']['zz_7293366434']['data'][2].decode('utf-8')[:4] == '2024')\n",
    "#print(dict_info_read['time_history']['zz_7293366434']['attrs']['finished'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section('Check read info files for unfinished runs again - Start')\n",
    "\n",
    "for stud in list_studies:\n",
    "    if (not dict_info_read['time_history'][stud]['attrs']['finished'])\\\n",
    "        and (dict_info_read['time_history'][stud]['data'][2].decode('utf-8')[:4] in ('2024', '2025')):\n",
    "        print('here 1', stud)\n",
    "    elif dict_info_read['time_history'][stud]['attrs']['finished']\\\n",
    "        and (dict_info_read['time_history'][stud]['data'][2].decode('utf-8')[:4] not in ('2024', '2025')):\n",
    "        print('here 1_2', stud)\n",
    "    elif dict_info_read['time_history'][stud]['attrs']['finished']\\\n",
    "        and (dict_info_read['time_history'][stud]['data'][2].decode('utf-8')[:4] in ('2024', '2025')):\n",
    "        #print('here 2')\n",
    "        continue\n",
    "    else:\n",
    "        print('here else', stud)\n",
    "        for key, value in dict_infos_read.items():\n",
    "            if '2024' == value['time_history'][stud]['data'][2].decode('utf-8')[:4]:\n",
    "                dict_info_read['time_history'][stud]['data'][1] = (value['time_history'][stud]['data'][1]+'.'.encode('utf-8'))[:-1]\n",
    "                dict_info_read['time_history'][stud]['data'][2] = (value['time_history'][stud]['data'][2]+'.'.encode('utf-8'))[:-1]\n",
    "                dict_info_read['time_history'][stud]['attrs']['finished'] = True\n",
    "print('if this is the only line printed, then all studies in the info file are finished')\n",
    "print_section('Check read info files for unfinished runs again - Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section('Check estimated runtime - Start')\n",
    "#print(dict_info_read['study'])\n",
    "study_read = dict_info_read['study']['data']\n",
    "study_read.dtype.names\n",
    "est_runts = study_read['sets']['estimated_runtime'] * 10 # 10 runs per set\n",
    "cum_est_runts = np.sum(est_runts)\n",
    "#print(est_runts)\n",
    "print('cummulative estimated runtime for 10000 samples each is [h]:', cum_est_runts/3600)\n",
    "del study_read, est_runts, cum_est_runts\n",
    "print_section('Check estimated runtime - Finished')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section('Extract run identifiers and separate into un-/finished - Start')\n",
    "array_identifiers, started_psets, finished_psets = h5py_funcs.inspections.extract_identifiers(dict_info_read = dict_info_read)\n",
    "\n",
    "print('merged dict:')\n",
    "print('  started_psets:', started_psets)\n",
    "print('  finished_psets:', finished_psets)\n",
    "print('  {p1}/{p2} p_sets have been started, \\n {p3}/{p2} p_sets have been finished'.format(p1=len(started_psets), p2=array_identifiers.shape[0], p3=len(finished_psets)))\n",
    "\n",
    "list_array_identifiers = []; list_started_psets = []; list_finished_psets = []\n",
    "appenders = [list_array_identifiers, list_started_psets, list_finished_psets]\n",
    "for d in dict_infos_read.values():\n",
    "    appendees = h5py_funcs.inspections.extract_identifiers(dict_info_read = d)\n",
    "#    print(len(appendees[0]), len(appendees[1]), len(appendees[2]))\n",
    "    assert (array_identifiers==appendees[0]).all()\n",
    "    list_array_identifiers.append(appendees[0])\n",
    "    if len(list_started_psets) > 0:\n",
    "#        print('here if')\n",
    "        #list_started_psets.append([ap for ap in appendees[1] if not ap in list_started_psets[-1]])\n",
    "        #list_finished_psets.append([ap for ap in appendees[2] if not ap in list_finished_psets[-1]])\n",
    "        list_started_psets.append([ap for ap in appendees[1] if not ap in [e for ll in list_started_psets for e in ll]])\n",
    "        list_finished_psets.append([ap for ap in appendees[2] if not ap in [e for ll in list_finished_psets for e in ll]])\n",
    "    else:\n",
    "#        print('here else')\n",
    "        list_started_psets.append(appendees[1])\n",
    "        list_finished_psets.append(appendees[2])\n",
    "print('from individual runs accumulated:')\n",
    "print('  list_started_psets:', list_started_psets)\n",
    "print('  list_finished_psets:', list_finished_psets)\n",
    "print('  {p1}/{p2} p_sets have been started, \\n {p3}/{p2} p_sets have been finished'.\\\n",
    "      format(p1=[len(l) for l in list_started_psets], p2=[len(l) for l in list_array_identifiers], p3=[len(l) for l in list_finished_psets]))\n",
    "print_section('Extract run identifiers and separate into un-/finished - Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section('Define read from/write to params - Start')\n",
    "#read_from = ['sample_data', 'pickle']\n",
    "#write_to = ['pickle', 'None']\n",
    "read_from = 'sample_data'\n",
    "write_to = 'pickle'\n",
    "print('  read_from:', read_from, ' , possibilities: \\'sample_data\\', \\'pickle\\'')\n",
    "print('  write_to:', write_to, ' , possibilities: \\'pickle\\', \\'None\\'')\n",
    "dict_for_df = {}\n",
    "print_section('Define read from/write to params - Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if read_from == 'sample_data':\n",
    "    print_section('Read from sample data - Start')\n",
    "\n",
    "    _reads = h5py_funcs.inspections.read_answers_to_dict_concurrent(samples_folder_name_path,list_finished_psets, num_procs=75, batch_size=5)\n",
    "    #for r in _reads:\n",
    "    #    print(r.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_reads[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_for_df = {k: v for e in _reads for k,v in e.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(dict_for_df.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "#\n",
    "# old version before concurrent read via joblib\n",
    "#\n",
    "######\n",
    "#\n",
    "#\n",
    "#if read_from == 'sample_data':\n",
    "#    print_section('Read from sample data - Start')\n",
    "#\n",
    "#    for sfnp, ids in zip(samples_folder_name_path,list_finished_psets):\n",
    "#        # samples_folder_name_path is a list of all the submissions to be read\n",
    "#        print('  reading from', sfnp)\n",
    "#        _tmp_study_name = sfnp.parent.stem\n",
    "#        print('  file name', _tmp_study_name)\n",
    "#        dict_for_df |= h5py_funcs.inspections.read_answers_to_dict(\\\n",
    "#            samples_folder_name_path=sfnp\\\n",
    "#            , array_identifiers=np.array(ids))#[::25]\n",
    "#    print(dict_for_df.__sizeof__()) # size in memory in bytes\n",
    "#    print(len(dict_for_df)) # number of keys in dict\n",
    "#    if write_to == 'pickle':\n",
    "#        with open(pathlib.Path('01_out',f'dict_for_df_{_tmp_study_name}.txt'), 'wb') as f:\n",
    "#            pickle.dump(dict_for_df, f, protocol=5)\n",
    "#    del _tmp_study_name\n",
    "#    print_section('Read from sample data - Finished')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "#\n",
    "# old version before concurrent read via joblib\n",
    "#\n",
    "######\n",
    "#\n",
    "#\n",
    "# if read_from == 'pickle':\n",
    "#    print_section('Read from pickle - Start')\n",
    "#    with open(pathlib.Path('01_out',f'dict_for_df_{study_name}.txt'), 'rb') as f:\n",
    "#        dict_for_df = pickle.load(f)\n",
    "#    print_section('Read from pickle - Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "#\n",
    "# old version before concurrent read via joblib\n",
    "#\n",
    "######\n",
    "#\n",
    "#\n",
    "#is_combine_pickled_dicts = False\n",
    "#if is_combine_pickled_dicts:\n",
    "#    print_section('Combine dicts read via Pickle - Start')\n",
    "#    in_dicts = ['01_out\\\\dict_for_df_sub_6_2.txt'\\\n",
    "#               ,'01_out\\\\dict_for_df_sub_6_2_2.txt']\n",
    "#    out_dicts = ['01_out\\\\dict_for_df_sub_6_2_combined.txt']\n",
    "#    out_dict = {}\n",
    "#    for in_d in in_dicts:\n",
    "#        print(f'  read {in_d}')\n",
    "#        with open(in_d, 'rb') as f:\n",
    "#            out_dict |= pickle.load(f)\n",
    "#    print('  finished reading all in_dicts')\n",
    "#    for key, val in out_dict.items():\n",
    "#        sample_set_length = (len(val['custom']['sampleset'].keys()))\n",
    "#        if sample_set_length != 10:\n",
    "#            print(key, sample_set_length)\n",
    "#    print_section('Combine dicts read via Pickle - Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "#\n",
    "# old version before concurrent read via joblib\n",
    "#\n",
    "######\n",
    "#\n",
    "#\n",
    "#if is_combine_pickled_dicts:\n",
    "#    print_section('Write combined dicts read via Pickle to file - Start')\n",
    "#    out_dicts = ['01_out\\\\dict_for_df_sub_5_3_combined.txt']\n",
    "#    with open(out_dicts[0], 'wb') as f:\n",
    "#        pickle.dump(out_dict, f, protocol=5)\n",
    "#    print_section('Write combined dicts read via Pickle to file - Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section('Obtain exact/correct solution via simulated annealing - Start')\n",
    "import ast\n",
    "import dwave.samplers\n",
    "\n",
    "num_particles = 5\n",
    "qubo = dict_info_read['qubos'][f'{num_particles}_{num_particles}']\n",
    "\n",
    "#sim_annealing_sample = dimod.samplers.ExactSolver().sample_qubo(\n",
    "#    {ast.literal_eval(key): value['data'] for key, value in qubo.items()})\n",
    "sim_annealing_sample = dwave.samplers.SimulatedAnnealingSampler().sample_qubo(\n",
    "    {ast.literal_eval(key): value['data'] for key, value in qubo.items()},\n",
    "    num_reads=10000)\n",
    "sim_annealing_sample = sim_annealing_sample.aggregate() # accumulates number of occurences\n",
    "#print(type(sim_annealing_sample))\n",
    "exact_sol = sim_annealing_sample.record\n",
    "exact_sol.sort(order='energy')\n",
    "#print(exact_sol)\n",
    "print_section('Obtain correct solution via simulated annealing - Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section('Extract success dict from info, samples, and correct solution - Start')\n",
    "n_samples_to_compare = 3\n",
    "n_exact_sols_to_compare = 3\n",
    "#dir_name_path_plots = './01_out/'\n",
    "\n",
    "success_dict = h5py_funcs.inspections.extract_success_dict(\\\n",
    "    dict_for_df = dict_for_df\\\n",
    "    , exact_sols = exact_sol\\\n",
    "    , n_samples_to_compare = n_samples_to_compare\\\n",
    "    , n_exact_sols_to_compare = n_exact_sols_to_compare)\n",
    "\n",
    "\n",
    "#for key in success_dict.keys():\n",
    "#    print(key, success_dict[key])\n",
    "print_section('Extract success dict from info, samples, and correct solution - Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section('Extract started parameter sets from success dict - Start')\n",
    "started_sets, study_matched_started_ids = h5py_funcs.inspections.extract_started_sets_from_success_dict(\\\n",
    "    success_dict = success_dict\\\n",
    "    , dict_info_read = dict_info_read)\n",
    "#print(started_sets)\n",
    "#print(study_matched_started_ids)\n",
    "#print(study_matched_started_ids.dtype)\n",
    "print_section('Extract started parameter sets from success dict - Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section('Generate sampler array for plots - Start')\n",
    "results_names = list(list(success_dict.values())[0].keys())\n",
    "to_remove = ['is_found_best_per_run', 'num_subs_per_run', 'num_samples_per_run', 'num_samples_per_sub_per_run', 'num_matched_per_run'\\\n",
    "             , 'num_matched_per_sub_per_run', 'num_samples_matched_per_run', 'num_samples_matched_per_sub_per_run', 'submissions', 'num_samples_is_found_best_per_run']\n",
    "for s in to_remove:\n",
    "    results_names.remove(s)\n",
    "#print(results_names)\n",
    "\n",
    "\n",
    "\n",
    "sampler_array,results_names, ids = h5py_funcs.inspections.generate_sampler_array_for_plots(\\\n",
    "    success_dict = success_dict\\\n",
    "    , results_names = results_names\\\n",
    "    , started_sets = started_sets\\\n",
    "    , study_matched_started_ids = study_matched_started_ids\\\n",
    "    , n_samples_to_compare = n_samples_to_compare\\\n",
    "    , n_exact_sols_to_compare = n_exact_sols_to_compare)\n",
    "\n",
    "#print(sampler_array.shape)\n",
    "#print(results_names)\n",
    "print_section('Generate sampler array for plots - Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print_section('Generate df from info file - Start')\n",
    "df1 = pd.DataFrame(study_matched_started_ids['identifiers'], columns=['identifiers'], dtype='str')\n",
    "df2 = pd.DataFrame().from_records(study_matched_started_ids['sets'])\n",
    "df = df1.join(df2)\n",
    "#print(df.head().to_string())\n",
    "print(df.columns)\n",
    "print_section('Generate df from info file - Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section('Generate df from success dict - Start')\n",
    "\n",
    "#print(list(success_dict['zz_8097006340'].keys())\n",
    "#print(success_dict['zz_8097006340'])\n",
    "\n",
    "df_success_dict = pd.DataFrame.from_dict(data=success_dict, orient='index')\n",
    "#print(df_success_dict.head().to_string())\n",
    "df_success_dict = df_success_dict.reset_index(names='identifiers')\n",
    "#print(df_success_dict.head().to_string())\n",
    "\n",
    "print_section('Generate df from success dict - Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section('Merge dfs from info file and success dict - Start')\n",
    "df_complete = df.merge(df_success_dict, on='identifiers')\n",
    "print(df_complete.head().to_string())\n",
    "print_section('Merge dfs from info file and success dict - Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('01_out/df_7_1.df', 'wb') as f:\n",
    "    pickle.dump(df_complete, f, protocol=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
